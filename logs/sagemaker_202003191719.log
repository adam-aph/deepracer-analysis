20:C 19 Mar 2020 16:19:22.624 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
20:C 19 Mar 2020 16:19:22.624 # Redis version=5.0.7, bits=64, commit=00000000, modified=0, pid=20, just started
20:C 19 Mar 2020 16:19:22.624 # Configuration loaded
                _._                                                  
           _.-``__ ''-._                                             
      _.-``    `.  `_.  ''-._           Redis 5.0.7 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._                                   
 (    '      ,       .-`  | `,    )     Running in standalone mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
 |    `-._   `._    /     _.-'    |     PID: 20
  `-._    `-._  `-./  _.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |           http://redis.io        
  `-._    `-._`-.__.-'_.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |                                  
  `-._    `-._`-.__.-'_.-'    _.-'                                   
      `-._    `-.__.-'    _.-'                                       
          `-._        _.-'                                           
              `-.__.-'                                               

20:M 19 Mar 2020 16:19:22.625 # WARNING: The TCP backlog setting of 512 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
20:M 19 Mar 2020 16:19:22.625 # Server initialized
20:M 19 Mar 2020 16:19:22.625 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
20:M 19 Mar 2020 16:19:22.625 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
20:M 19 Mar 2020 16:19:22.625 * Ready to accept connections
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2020-03-19 16:19:24,243 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training
2020-03-19 16:19:24,400 sagemaker-containers INFO     Invoking user script

Training Env:

{
    "additional_framework_parameters": {
        "sagemaker_estimator": "RLEstimator"
    },
    "channel_input_dirs": {},
    "current_host": "algo-1-mnla2",
    "framework_module": "sagemaker_tensorflow_container.training:main",
    "hosts": [
        "algo-1-mnla2"
    ],
    "hyperparameters": {
        "s3_bucket": "bucket",
        "s3_prefix": "current",
        "aws_region": "us-east-1",
        "model_metadata_s3_key": "s3://bucket/custom_files/model_metadata.json",
        "RLCOACH_PRESET": "deepracer",
        "batch_size": 64,
        "beta_entropy": 0.01,
        "discount_factor": 0.999,
        "e_greedy_value": 0.05,
        "epsilon_steps": 10000,
        "exploration_type": "categorical",
        "loss_type": "mean squared error",
        "lr": 1e-05,
        "num_episodes_between_training": 20,
        "num_epochs": 10,
        "stack_size": 1,
        "term_cond_avg_score": 100000.0,
        "term_cond_max_episodes": 100000
    },
    "input_config_dir": "/opt/ml/input/config",
    "input_data_config": {},
    "input_dir": "/opt/ml/input",
    "is_master": true,
    "job_name": "current",
    "log_level": 20,
    "master_hostname": "algo-1-mnla2",
    "model_dir": "/opt/ml/model",
    "module_dir": "s3://bucket/current/source/sourcedir.tar.gz",
    "module_name": "training_worker",
    "network_interface_name": "eth0",
    "num_cpus": 8,
    "num_gpus": 1,
    "output_data_dir": "/opt/ml/output/data",
    "output_dir": "/opt/ml/output",
    "output_intermediate_dir": "/opt/ml/output/intermediate",
    "resource_config": {
        "current_host": "algo-1-mnla2",
        "hosts": [
            "algo-1-mnla2"
        ]
    },
    "user_entry_point": "training_worker.py"
}

Environment variables:

SM_HOSTS=["algo-1-mnla2"]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":64,"beta_entropy":0.01,"discount_factor":0.999,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"mean squared error","lr":1e-05,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":20,"num_epochs":10,"s3_bucket":"bucket","s3_prefix":"current","stack_size":1,"term_cond_avg_score":100000.0,"term_cond_max_episodes":100000}
SM_USER_ENTRY_POINT=training_worker.py
SM_FRAMEWORK_PARAMS={"sagemaker_estimator":"RLEstimator"}
SM_RESOURCE_CONFIG={"current_host":"algo-1-mnla2","hosts":["algo-1-mnla2"]}
SM_INPUT_DATA_CONFIG={}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=[]
SM_CURRENT_HOST=algo-1-mnla2
SM_MODULE_NAME=training_worker
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=8
SM_NUM_GPUS=1
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://bucket/current/source/sourcedir.tar.gz
SM_TRAINING_ENV={"additional_framework_parameters":{"sagemaker_estimator":"RLEstimator"},"channel_input_dirs":{},"current_host":"algo-1-mnla2","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1-mnla2"],"hyperparameters":{"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":64,"beta_entropy":0.01,"discount_factor":0.999,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"mean squared error","lr":1e-05,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":20,"num_epochs":10,"s3_bucket":"bucket","s3_prefix":"current","stack_size":1,"term_cond_avg_score":100000.0,"term_cond_max_episodes":100000},"input_config_dir":"/opt/ml/input/config","input_data_config":{},"input_dir":"/opt/ml/input","is_master":true,"job_name":"current","log_level":20,"master_hostname":"algo-1-mnla2","model_dir":"/opt/ml/model","module_dir":"s3://bucket/current/source/sourcedir.tar.gz","module_name":"training_worker","network_interface_name":"eth0","num_cpus":8,"num_gpus":1,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1-mnla2","hosts":["algo-1-mnla2"]},"user_entry_point":"training_worker.py"}
SM_USER_ARGS=["--RLCOACH_PRESET","deepracer","--aws_region","us-east-1","--batch_size","64","--beta_entropy","0.01","--discount_factor","0.999","--e_greedy_value","0.05","--epsilon_steps","10000","--exploration_type","categorical","--loss_type","mean squared error","--lr","1e-05","--model_metadata_s3_key","s3://bucket/custom_files/model_metadata.json","--num_episodes_between_training","20","--num_epochs","10","--s3_bucket","bucket","--s3_prefix","current","--stack_size","1","--term_cond_avg_score","100000.0","--term_cond_max_episodes","100000"]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_HP_S3_BUCKET=bucket
SM_HP_S3_PREFIX=current
SM_HP_AWS_REGION=us-east-1
SM_HP_MODEL_METADATA_S3_KEY=s3://bucket/custom_files/model_metadata.json
SM_HP_RLCOACH_PRESET=deepracer
SM_HP_BATCH_SIZE=64
SM_HP_BETA_ENTROPY=0.01
SM_HP_DISCOUNT_FACTOR=0.999
SM_HP_E_GREEDY_VALUE=0.05
SM_HP_EPSILON_STEPS=10000
SM_HP_EXPLORATION_TYPE=categorical
SM_HP_LOSS_TYPE=mean squared error
SM_HP_LR=1e-05
SM_HP_NUM_EPISODES_BETWEEN_TRAINING=20
SM_HP_NUM_EPOCHS=10
SM_HP_STACK_SIZE=1
SM_HP_TERM_COND_AVG_SCORE=100000.0
SM_HP_TERM_COND_MAX_EPISODES=100000
PYTHONPATH=/usr/local/bin:/opt/amazon:/opt/ml/code:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages

Invoking script with the following command:

/usr/bin/python training_worker.py --RLCOACH_PRESET deepracer --aws_region us-east-1 --batch_size 64 --beta_entropy 0.01 --discount_factor 0.999 --e_greedy_value 0.05 --epsilon_steps 10000 --exploration_type categorical --loss_type mean squared error --lr 1e-05 --model_metadata_s3_key s3://bucket/custom_files/model_metadata.json --num_episodes_between_training 20 --num_epochs 10 --s3_bucket bucket --s3_prefix current --stack_size 1 --term_cond_avg_score 100000.0 --term_cond_max_episodes 100000


/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
S3 bucket: bucket 
 S3 prefix: current 
 S3 endpoint URL: http://minio:9000
Initializing SageS3Client...
Successfully downloaded model metadata from custom_files/model_metadata.json.
Sensor list ['STEREO_CAMERAS'], network DEEP_CONVOLUTIONAL_NETWORK_SHALLOW, simapp_version 2.0
Loaded action space from file: [{'steering_angle': -30, 'speed': 1.0, 'index': 0}, {'steering_angle': -20, 'speed': 1.3333333333333333, 'index': 1}, {'steering_angle': -10, 'speed': 2, 'index': 2}, {'steering_angle': 0, 'speed': 2.5, 'index': 3}, {'steering_angle': 10, 'speed': 2, 'index': 4}, {'steering_angle': 20, 'speed': 1.3333333333333333, 'index': 5}, {'steering_angle': 30, 'speed': 1.0, 'index': 6}]
Using the following hyper-parameters
{
  "batch_size": 64,
  "beta_entropy": 0.01,
  "discount_factor": 0.999,
  "e_greedy_value": 0.05,
  "epsilon_steps": 10000,
  "exploration_type": "categorical",
  "loss_type": "mean squared error",
  "lr": 1e-05,
  "num_episodes_between_training": 20,
  "num_epochs": 10,
  "stack_size": 1,
  "term_cond_avg_score": 100000.0,
  "term_cond_max_episodes": 100000
}
Uploaded hyperparameters.json to S3
Uploaded IP address information to S3: 172.18.0.5
## Creating graph - name: MultiAgentGraphManager
## Creating agent - name: agent
INFO:tensorflow:./checkpoint/0_Step-0.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/0_Step-0.ckpt']
Uploaded 3 files for checkpoint 0 in 0.38 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_0.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_0.pb to /opt/ml/model/agent/model.pb.
Uploaded 3 files for checkpoint 0 in 0.71 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_0.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_0.pb to /opt/ml/model/agent/model.pb.
DoorMan: installing SIGINT, SIGTERM
Training> Name=main_level/agent, Worker=0, Episode=1, Total reward=95.02, Steps=19, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=2, Total reward=38.01, Steps=27, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=3, Total reward=38.01, Steps=35, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=4, Total reward=100.03, Steps=56, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=5, Total reward=53.01, Steps=67, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=6, Total reward=38.01, Steps=75, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=7, Total reward=97.02, Steps=95, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=8, Total reward=67.02, Steps=109, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=9, Total reward=48.01, Steps=119, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=10, Total reward=62.02, Steps=132, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=11, Total reward=52.01, Steps=143, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=12, Total reward=42.01, Steps=152, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=13, Total reward=47.01, Steps=162, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=14, Total reward=57.02, Steps=174, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=15, Total reward=78.02, Steps=190, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=16, Total reward=83.03, Steps=208, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=17, Total reward=67.02, Steps=222, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=18, Total reward=42.01, Steps=231, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=19, Total reward=38.01, Steps=239, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=20, Total reward=37.01, Steps=247, Training iteration=0
Policy training> Surrogate loss=0.016513844951987267, KL divergence=2.324254637642298e-05, Entropy=1.944480299949646, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.029855376109480858, KL divergence=0.00016526073159184307, Entropy=1.943804383277893, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.01999438740313053, KL divergence=0.00046249674051068723, Entropy=1.9429794549942017, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.007859398610889912, KL divergence=0.0009564603678882122, Entropy=1.9418309926986694, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.007507254835218191, KL divergence=0.0016493052244186401, Entropy=1.940531611442566, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030067821964621544, KL divergence=0.0023564298171550035, Entropy=1.9394124746322632, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03405115380883217, KL divergence=0.0032274627592414618, Entropy=1.9379349946975708, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.031213248148560524, KL divergence=0.004340326879173517, Entropy=1.9361845254898071, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.06066560372710228, KL divergence=0.005732301622629166, Entropy=1.9341896772384644, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0313943475484848, KL divergence=0.007453501224517822, Entropy=1.9317961931228638, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/1_Step-247.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/1_Step-247.ckpt']
Uploaded 3 files for checkpoint 1 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_1.pb
Unable to parse best checkpoint data: No checkpoint recorded, using last                     checkpoint instead
Unable to find the best checkpoint number. Getting the last checkpoint number
Best checkpoint number: 0, Last checkpoint number: 0
Copying the frozen checkpoint from ./frozen_models/agent/model_0.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=21, Total reward=48.01, Steps=257, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=22, Total reward=58.01, Steps=269, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=23, Total reward=42.01, Steps=278, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=24, Total reward=42.01, Steps=287, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=25, Total reward=43.01, Steps=296, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=26, Total reward=38.01, Steps=304, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=27, Total reward=47.01, Steps=314, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=28, Total reward=68.02, Steps=328, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=29, Total reward=72.02, Steps=343, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=30, Total reward=57.02, Steps=355, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=31, Total reward=28.01, Steps=361, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=32, Total reward=47.01, Steps=371, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=33, Total reward=43.01, Steps=380, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=34, Total reward=42.01, Steps=389, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=35, Total reward=42.01, Steps=398, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=36, Total reward=78.02, Steps=414, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=37, Total reward=93.02, Steps=433, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=38, Total reward=51.02, Steps=444, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=39, Total reward=47.01, Steps=454, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=40, Total reward=65.02, Steps=468, Training iteration=1
Policy training> Surrogate loss=-0.046304840594530106, KL divergence=1.3218593267083634e-05, Entropy=1.9290345907211304, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.003560160519555211, KL divergence=0.00012813223293051124, Entropy=1.9264856576919556, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.03740191087126732, KL divergence=0.00032562631531618536, Entropy=1.9252996444702148, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02117273211479187, KL divergence=0.0006241825758479536, Entropy=1.9245457649230957, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.021667271852493286, KL divergence=0.0011111932108178735, Entropy=1.9234765768051147, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.024442680180072784, KL divergence=0.0017563425935804844, Entropy=1.9226754903793335, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.00740206241607666, KL divergence=0.002578602870926261, Entropy=1.9221261739730835, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=0.007821785286068916, KL divergence=0.0037027534563094378, Entropy=1.9203649759292603, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.020337270572781563, KL divergence=0.0051360842771828175, Entropy=1.9184373617172241, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=0.04392406716942787, KL divergence=0.006809635553508997, Entropy=1.9160633087158203, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/2_Step-468.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/2_Step-468.ckpt']
Uploaded 3 files for checkpoint 2 in 0.54 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_2.pb
Best checkpoint number: 0, Last checkpoint number: 0
Copying the frozen checkpoint from ./frozen_models/agent/model_0.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=41, Total reward=70.01, Steps=482, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=42, Total reward=57.02, Steps=494, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=43, Total reward=43.01, Steps=503, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=44, Total reward=52.01, Steps=514, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=45, Total reward=57.02, Steps=526, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=46, Total reward=53.01, Steps=537, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=47, Total reward=76.02, Steps=553, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=48, Total reward=62.02, Steps=566, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=49, Total reward=67.02, Steps=580, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=50, Total reward=70.01, Steps=594, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=51, Total reward=37.01, Steps=602, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=52, Total reward=42.01, Steps=611, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=53, Total reward=143.03, Steps=640, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=54, Total reward=63.02, Steps=653, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=55, Total reward=68.02, Steps=667, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=56, Total reward=32.01, Steps=674, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=57, Total reward=47.01, Steps=684, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=58, Total reward=57.02, Steps=696, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=59, Total reward=42.01, Steps=705, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=60, Total reward=50.01, Steps=715, Training iteration=2
Policy training> Surrogate loss=-0.000651925802230835, KL divergence=9.648779268900398e-06, Entropy=1.9176496267318726, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.029481565579771996, KL divergence=5.6004413636401296e-05, Entropy=1.9167224168777466, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.0303686261177063, KL divergence=8.285974035970867e-05, Entropy=1.9167808294296265, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.020069092512130737, KL divergence=9.42614278756082e-05, Entropy=1.9168440103530884, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.0009158949251286685, KL divergence=0.00017527463205624372, Entropy=1.9150928258895874, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.05961598455905914, KL divergence=0.0004293259989935905, Entropy=1.913351058959961, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.06677571684122086, KL divergence=0.0008896075305528939, Entropy=1.9123821258544922, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02907600998878479, KL divergence=0.0016580201918259263, Entropy=1.9106749296188354, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.010679860599339008, KL divergence=0.0029494224581867456, Entropy=1.9079036712646484, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.06009852886199951, KL divergence=0.004636965226382017, Entropy=1.904576301574707, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/3_Step-715.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/3_Step-715.ckpt']
Uploaded 3 files for checkpoint 3 in 0.53 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_3.pb
Best checkpoint number: 0, Last checkpoint number: 1
Copying the frozen checkpoint from ./frozen_models/agent/model_0.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'1'}
Training> Name=main_level/agent, Worker=0, Episode=61, Total reward=38.01, Steps=723, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=62, Total reward=63.02, Steps=736, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=63, Total reward=42.01, Steps=745, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=64, Total reward=38.01, Steps=753, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=65, Total reward=63.02, Steps=766, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=66, Total reward=42.01, Steps=775, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=67, Total reward=61.02, Steps=788, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=68, Total reward=43.01, Steps=797, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=69, Total reward=48.01, Steps=807, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=70, Total reward=78.02, Steps=823, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=71, Total reward=86.03, Steps=842, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=72, Total reward=43.01, Steps=851, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=73, Total reward=123.03, Steps=877, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=74, Total reward=48.01, Steps=887, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=75, Total reward=73.02, Steps=902, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=76, Total reward=38.01, Steps=910, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=77, Total reward=63.02, Steps=923, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=78, Total reward=47.01, Steps=933, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=79, Total reward=37.01, Steps=941, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=80, Total reward=41.01, Steps=950, Training iteration=3
Policy training> Surrogate loss=-0.03999531641602516, KL divergence=3.996423401986249e-05, Entropy=1.9000024795532227, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.05427125096321106, KL divergence=0.00027372673503123224, Entropy=1.8969680070877075, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.048376575112342834, KL divergence=0.0005228896043263376, Entropy=1.8973712921142578, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.015510712750256062, KL divergence=0.0008771868306212127, Entropy=1.8969053030014038, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.0010581662645563483, KL divergence=0.0011920916149392724, Entropy=1.8972526788711548, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.01716805063188076, KL divergence=0.00147404579911381, Entropy=1.8987218141555786, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=0.0040474929846823215, KL divergence=0.0019164500990882516, Entropy=1.9008339643478394, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.016818055883049965, KL divergence=0.002518661320209503, Entropy=1.9047194719314575, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02053137682378292, KL divergence=0.0032435767352581024, Entropy=1.9076381921768188, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=0.0067978850565850735, KL divergence=0.004222289193421602, Entropy=1.9106216430664062, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/4_Step-950.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/4_Step-950.ckpt']
Uploaded 3 files for checkpoint 4 in 0.53 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_4.pb
Best checkpoint number: 0, Last checkpoint number: 2
Copying the frozen checkpoint from ./frozen_models/agent/model_0.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'2'}
Training> Name=main_level/agent, Worker=0, Episode=81, Total reward=128.03, Steps=976, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=82, Total reward=36.01, Steps=984, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=83, Total reward=42.01, Steps=993, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=84, Total reward=33.01, Steps=1000, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=85, Total reward=58.01, Steps=1012, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=86, Total reward=38.01, Steps=1020, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=87, Total reward=167.04, Steps=1054, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=88, Total reward=61.02, Steps=1067, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=89, Total reward=62.02, Steps=1080, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=90, Total reward=52.01, Steps=1091, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=91, Total reward=68.02, Steps=1105, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=92, Total reward=52.01, Steps=1116, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=93, Total reward=78.02, Steps=1132, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=94, Total reward=46.01, Steps=1142, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=95, Total reward=77.02, Steps=1158, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=96, Total reward=47.01, Steps=1168, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=97, Total reward=118.03, Steps=1192, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=98, Total reward=52.01, Steps=1203, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=99, Total reward=83.02, Steps=1220, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=100, Total reward=37.01, Steps=1228, Training iteration=4
Policy training> Surrogate loss=-0.0021208561956882477, KL divergence=6.480111187556759e-05, Entropy=1.9154787063598633, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.005194853059947491, KL divergence=0.0005997011321596801, Entropy=1.9201222658157349, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02548147737979889, KL divergence=0.0015420347917824984, Entropy=1.923569917678833, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.014820095151662827, KL divergence=0.0029796482995152473, Entropy=1.9256017208099365, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.003094911575317383, KL divergence=0.004909412004053593, Entropy=1.9278227090835571, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.019705843180418015, KL divergence=0.007253827527165413, Entropy=1.9293324947357178, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.010541584342718124, KL divergence=0.009180589579045773, Entropy=1.9300050735473633, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0002345680259168148, KL divergence=0.009916643612086773, Entropy=1.9297879934310913, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.028523296117782593, KL divergence=0.010362181812524796, Entropy=1.9290392398834229, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03335922583937645, KL divergence=0.011038076132535934, Entropy=1.9277628660202026, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/5_Step-1228.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/5_Step-1228.ckpt']
Uploaded 3 files for checkpoint 5 in 0.55 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_5.pb
Best checkpoint number: 3, Last checkpoint number: 3
Copying the frozen checkpoint from ./frozen_models/agent/model_3.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'0'}
Training> Name=main_level/agent, Worker=0, Episode=101, Total reward=57.02, Steps=1240, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=102, Total reward=62.02, Steps=1253, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=103, Total reward=57.02, Steps=1265, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=104, Total reward=42.01, Steps=1274, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=105, Total reward=43.01, Steps=1283, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=106, Total reward=42.01, Steps=1292, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=107, Total reward=52.01, Steps=1303, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=108, Total reward=75.02, Steps=1319, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=109, Total reward=58.01, Steps=1331, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=110, Total reward=42.01, Steps=1340, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=111, Total reward=63.02, Steps=1353, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=112, Total reward=103.02, Steps=1374, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=113, Total reward=57.02, Steps=1386, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=114, Total reward=58.01, Steps=1398, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=115, Total reward=67.02, Steps=1412, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=116, Total reward=33.01, Steps=1419, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=117, Total reward=58.01, Steps=1431, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=118, Total reward=68.02, Steps=1445, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=119, Total reward=52.01, Steps=1456, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=120, Total reward=67.02, Steps=1470, Training iteration=5
Policy training> Surrogate loss=-0.033163558691740036, KL divergence=4.654091299016727e-06, Entropy=1.9262458086013794, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.0031638729851692915, KL divergence=3.5484546970110387e-05, Entropy=1.9257138967514038, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.01617472432553768, KL divergence=0.0001061986549757421, Entropy=1.9254350662231445, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.03545685485005379, KL divergence=0.00020746293012052774, Entropy=1.9254308938980103, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.08400396257638931, KL divergence=0.0004413238202687353, Entropy=1.9249471426010132, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.012890477664768696, KL divergence=0.0008616098202764988, Entropy=1.9247993230819702, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03653908893465996, KL divergence=0.0013908111723139882, Entropy=1.9248989820480347, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.08788195997476578, KL divergence=0.0019806993659585714, Entropy=1.925132155418396, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=0.024073714390397072, KL divergence=0.002930210903286934, Entropy=1.9234377145767212, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=0.03339576721191406, KL divergence=0.003955383785068989, Entropy=1.9224157333374023, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/6_Step-1470.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/6_Step-1470.ckpt']
Uploaded 3 files for checkpoint 6 in 0.50 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_6.pb
Best checkpoint number: 4, Last checkpoint number: 4
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'0'}
Training> Name=main_level/agent, Worker=0, Episode=121, Total reward=55.01, Steps=1481, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=122, Total reward=62.02, Steps=1494, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=123, Total reward=72.02, Steps=1509, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=124, Total reward=53.01, Steps=1520, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=125, Total reward=82.02, Steps=1537, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=126, Total reward=38.01, Steps=1545, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=127, Total reward=68.02, Steps=1559, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=128, Total reward=82.02, Steps=1576, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=129, Total reward=43.01, Steps=1585, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=130, Total reward=47.01, Steps=1595, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=131, Total reward=58.01, Steps=1607, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=132, Total reward=42.01, Steps=1616, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=133, Total reward=62.02, Steps=1629, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=134, Total reward=92.02, Steps=1648, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=135, Total reward=67.02, Steps=1662, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=136, Total reward=33.01, Steps=1669, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=137, Total reward=52.01, Steps=1680, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=138, Total reward=64.01, Steps=1693, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=139, Total reward=71.02, Steps=1708, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=140, Total reward=97.02, Steps=1728, Training iteration=6
Policy training> Surrogate loss=0.007648431695997715, KL divergence=2.1667357941623777e-05, Entropy=1.9217145442962646, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.0041999779641628265, KL divergence=0.00028332386864349246, Entropy=1.918999195098877, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.004974067211151123, KL divergence=0.0009494436671957374, Entropy=1.915342092514038, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.0076582832261919975, KL divergence=0.0021326649002730846, Entropy=1.9108788967132568, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.019602041691541672, KL divergence=0.004088112153112888, Entropy=1.9054138660430908, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.01083669438958168, KL divergence=0.006788041442632675, Entropy=1.8993971347808838, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.01891586184501648, KL divergence=0.009888619184494019, Entropy=1.8932374715805054, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.023202717304229736, KL divergence=0.011367268860340118, Entropy=1.8899493217468262, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.01840076595544815, KL divergence=0.010698728263378143, Entropy=1.8910760879516602, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.012919487431645393, KL divergence=0.009323032572865486, Entropy=1.8942080736160278, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/7_Step-1728.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/7_Step-1728.ckpt']
Uploaded 3 files for checkpoint 7 in 0.52 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_7.pb
Best checkpoint number: 4, Last checkpoint number: 5
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'5'}
Training> Name=main_level/agent, Worker=0, Episode=141, Total reward=58.01, Steps=1740, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=142, Total reward=37.01, Steps=1748, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=143, Total reward=67.02, Steps=1762, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=144, Total reward=38.01, Steps=1770, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=145, Total reward=61.02, Steps=1783, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=146, Total reward=58.01, Steps=1795, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=147, Total reward=52.01, Steps=1806, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=148, Total reward=58.01, Steps=1818, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=149, Total reward=48.01, Steps=1828, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=150, Total reward=62.02, Steps=1841, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=151, Total reward=53.01, Steps=1852, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=152, Total reward=33.01, Steps=1859, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=153, Total reward=38.01, Steps=1867, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=154, Total reward=62.02, Steps=1880, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=155, Total reward=83.02, Steps=1897, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=156, Total reward=37.01, Steps=1905, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=157, Total reward=60.02, Steps=1918, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=158, Total reward=42.01, Steps=1927, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=159, Total reward=33.01, Steps=1934, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=160, Total reward=41.01, Steps=1943, Training iteration=7
Policy training> Surrogate loss=0.02345871366560459, KL divergence=9.816945748752914e-06, Entropy=1.8929075002670288, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.0009470855002291501, KL divergence=4.761465243063867e-05, Entropy=1.8935102224349976, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.025259606540203094, KL divergence=0.00016142726235557348, Entropy=1.8930983543395996, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.005088781472295523, KL divergence=0.00039155097329057753, Entropy=1.893358826637268, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0018729703733697534, KL divergence=0.0007322141318581998, Entropy=1.8914295434951782, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02203463576734066, KL divergence=0.0012039021821692586, Entropy=1.888723373413086, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=0.005071315914392471, KL divergence=0.0020554736256599426, Entropy=1.8843787908554077, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=0.010629897005856037, KL divergence=0.003182176500558853, Entropy=1.88162362575531, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.08156910538673401, KL divergence=0.004387926310300827, Entropy=1.8786331415176392, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.006610984448343515, KL divergence=0.006275059189647436, Entropy=1.8725404739379883, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/8_Step-1943.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/8_Step-1943.ckpt']
Uploaded 3 files for checkpoint 8 in 0.55 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_8.pb
Best checkpoint number: 4, Last checkpoint number: 6
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'3'}
Training> Name=main_level/agent, Worker=0, Episode=161, Total reward=57.02, Steps=1955, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=162, Total reward=47.01, Steps=1965, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=163, Total reward=80.02, Steps=1982, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=164, Total reward=62.02, Steps=1995, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=165, Total reward=57.02, Steps=2007, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=166, Total reward=52.01, Steps=2018, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=167, Total reward=67.02, Steps=2032, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=168, Total reward=58.01, Steps=2044, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=169, Total reward=37.01, Steps=2052, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=170, Total reward=80.02, Steps=2068, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=171, Total reward=57.02, Steps=2080, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=172, Total reward=108.02, Steps=2102, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=173, Total reward=43.01, Steps=2111, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=174, Total reward=52.01, Steps=2122, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=175, Total reward=38.01, Steps=2130, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=176, Total reward=37.01, Steps=2138, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=177, Total reward=65.02, Steps=2152, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=178, Total reward=47.01, Steps=2162, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=179, Total reward=45.02, Steps=2172, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=180, Total reward=76.02, Steps=2188, Training iteration=8
Policy training> Surrogate loss=0.03842766582965851, KL divergence=1.2259483810339589e-05, Entropy=1.8680795431137085, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.005487417336553335, KL divergence=9.000881254905835e-05, Entropy=1.8673840761184692, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.013114821165800095, KL divergence=0.00047036437899805605, Entropy=1.8710774183273315, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.020557330921292305, KL divergence=0.0016150152077898383, Entropy=1.8758668899536133, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.01394103467464447, KL divergence=0.0037694312632083893, Entropy=1.8817671537399292, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02318890392780304, KL divergence=0.006827361881732941, Entropy=1.8867141008377075, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.009518846869468689, KL divergence=0.008842994458973408, Entropy=1.889726996421814, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.04410584270954132, KL divergence=0.00927001889795065, Entropy=1.8909751176834106, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.08509262651205063, KL divergence=0.00846730638295412, Entropy=1.891455054283142, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.04877558350563049, KL divergence=0.008166386745870113, Entropy=1.8895044326782227, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/9_Step-2188.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/9_Step-2188.ckpt']
Uploaded 3 files for checkpoint 9 in 0.49 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_9.pb
Best checkpoint number: 4, Last checkpoint number: 7
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'6'}
Training> Name=main_level/agent, Worker=0, Episode=181, Total reward=37.01, Steps=2196, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=182, Total reward=97.02, Steps=2216, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=183, Total reward=38.01, Steps=2224, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=184, Total reward=112.03, Steps=2247, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=185, Total reward=47.01, Steps=2257, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=186, Total reward=37.01, Steps=2265, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=187, Total reward=57.02, Steps=2277, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=188, Total reward=87.02, Steps=2295, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=189, Total reward=37.01, Steps=2303, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=190, Total reward=88.02, Steps=2321, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=191, Total reward=30.01, Steps=2327, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=192, Total reward=71.02, Steps=2342, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=193, Total reward=53.01, Steps=2353, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=194, Total reward=58.02, Steps=2366, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=195, Total reward=33.01, Steps=2373, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=196, Total reward=42.01, Steps=2382, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=197, Total reward=72.02, Steps=2397, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=198, Total reward=57.02, Steps=2409, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=199, Total reward=42.01, Steps=2418, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=200, Total reward=90.02, Steps=2437, Training iteration=9
Policy training> Surrogate loss=0.012849118560552597, KL divergence=3.6002096749143675e-05, Entropy=1.890283226966858, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.010690906085073948, KL divergence=0.0003441644075792283, Entropy=1.893144965171814, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02745021879673004, KL divergence=0.0010544854449108243, Entropy=1.8946924209594727, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.017388230189681053, KL divergence=0.0021614043507725, Entropy=1.8976656198501587, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.09154796600341797, KL divergence=0.003756182035431266, Entropy=1.8994272947311401, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.07035980373620987, KL divergence=0.005889939609915018, Entropy=1.9001930952072144, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=0.011735320091247559, KL divergence=0.008630235679447651, Entropy=1.9016743898391724, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.006261707749217749, KL divergence=0.011393546126782894, Entropy=1.9032057523727417, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=0.05753280594944954, KL divergence=0.014017083682119846, Entropy=1.9036073684692383, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0014306356897577643, KL divergence=0.015218016691505909, Entropy=1.9059568643569946, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/10_Step-2437.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/10_Step-2437.ckpt']
Uploaded 3 files for checkpoint 10 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_10.pb
Best checkpoint number: 4, Last checkpoint number: 8
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'7'}
Training> Name=main_level/agent, Worker=0, Episode=201, Total reward=62.02, Steps=2450, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=202, Total reward=95.02, Steps=2470, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=203, Total reward=47.01, Steps=2480, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=204, Total reward=53.01, Steps=2491, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=205, Total reward=68.02, Steps=2505, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=206, Total reward=48.01, Steps=2515, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=207, Total reward=97.02, Steps=2535, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=208, Total reward=83.02, Steps=2552, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=209, Total reward=47.01, Steps=2562, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=210, Total reward=65.01, Steps=2575, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=211, Total reward=47.01, Steps=2585, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=212, Total reward=141.04, Steps=2615, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=213, Total reward=47.01, Steps=2625, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=214, Total reward=53.01, Steps=2636, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=215, Total reward=38.01, Steps=2644, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=216, Total reward=113.03, Steps=2667, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=217, Total reward=38.01, Steps=2675, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=218, Total reward=46.01, Steps=2685, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=219, Total reward=70.01, Steps=2699, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=220, Total reward=37.01, Steps=2707, Training iteration=10
Policy training> Surrogate loss=-0.01937161013484001, KL divergence=3.068682781304233e-05, Entropy=1.91038179397583, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.006957164034247398, KL divergence=0.00017685734201222658, Entropy=1.9112054109573364, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.0064536286517977715, KL divergence=0.00043329052277840674, Entropy=1.9117193222045898, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.008748268708586693, KL divergence=0.0009509102674201131, Entropy=1.9105108976364136, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.00025067292153835297, KL divergence=0.0018437139224261045, Entropy=1.9083080291748047, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.007712766528129578, KL divergence=0.003117357147857547, Entropy=1.9058974981307983, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.01338721252977848, KL divergence=0.004576356150209904, Entropy=1.9033093452453613, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=0.012776866555213928, KL divergence=0.005890897009521723, Entropy=1.9001898765563965, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.002405881881713867, KL divergence=0.006836624350398779, Entropy=1.8975954055786133, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.010415671393275261, KL divergence=0.007303152233362198, Entropy=1.8957897424697876, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/11_Step-2707.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/11_Step-2707.ckpt']
Uploaded 3 files for checkpoint 11 in 0.47 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_11.pb
Best checkpoint number: 4, Last checkpoint number: 9
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'8'}
Training> Name=main_level/agent, Worker=0, Episode=221, Total reward=43.01, Steps=2716, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=222, Total reward=53.01, Steps=2727, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=223, Total reward=77.02, Steps=2744, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=224, Total reward=38.01, Steps=2752, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=225, Total reward=48.01, Steps=2762, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=226, Total reward=42.01, Steps=2771, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=227, Total reward=48.01, Steps=2781, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=228, Total reward=107.03, Steps=2804, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=229, Total reward=43.01, Steps=2813, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=230, Total reward=42.01, Steps=2822, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=231, Total reward=52.01, Steps=2833, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=232, Total reward=46.01, Steps=2843, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=233, Total reward=48.01, Steps=2853, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=234, Total reward=64.02, Steps=2867, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=235, Total reward=39.01, Steps=2875, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=236, Total reward=63.02, Steps=2888, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=237, Total reward=51.02, Steps=2899, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=238, Total reward=57.02, Steps=2911, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=239, Total reward=37.01, Steps=2919, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=240, Total reward=98.02, Steps=2939, Training iteration=11
Policy training> Surrogate loss=-0.012981717474758625, KL divergence=1.167757454823004e-05, Entropy=1.8928464651107788, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.017244430258870125, KL divergence=0.00012548238737508655, Entropy=1.8941036462783813, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.0772254467010498, KL divergence=0.0004665780288632959, Entropy=1.8942880630493164, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.010563108138740063, KL divergence=0.0008967379690147936, Entropy=1.8958693742752075, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.017794042825698853, KL divergence=0.0014151382492855191, Entropy=1.8957839012145996, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.012672364711761475, KL divergence=0.0019383225589990616, Entropy=1.8963111639022827, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=0.01695372723042965, KL divergence=0.0026714911218732595, Entropy=1.8958274126052856, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=0.00490095978602767, KL divergence=0.0036444205325096846, Entropy=1.8969225883483887, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=0.0302792489528656, KL divergence=0.005042418837547302, Entropy=1.8974918127059937, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=0.013535350561141968, KL divergence=0.006650223862379789, Entropy=1.8957505226135254, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/12_Step-2939.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/12_Step-2939.ckpt']
Uploaded 3 files for checkpoint 12 in 0.53 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_12.pb
Best checkpoint number: 4, Last checkpoint number: 10
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'9'}
Training> Name=main_level/agent, Worker=0, Episode=241, Total reward=38.01, Steps=2947, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=242, Total reward=53.01, Steps=2958, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=243, Total reward=47.01, Steps=2968, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=244, Total reward=96.02, Steps=2988, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=245, Total reward=47.01, Steps=2998, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=246, Total reward=38.01, Steps=3006, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=247, Total reward=51.02, Steps=3017, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=248, Total reward=38.01, Steps=3025, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=249, Total reward=61.02, Steps=3038, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=250, Total reward=53.01, Steps=3049, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=251, Total reward=68.02, Steps=3063, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=252, Total reward=67.02, Steps=3077, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=253, Total reward=58.01, Steps=3089, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=254, Total reward=52.01, Steps=3100, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=255, Total reward=62.02, Steps=3113, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=256, Total reward=42.01, Steps=3122, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=257, Total reward=91.02, Steps=3141, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=258, Total reward=83.02, Steps=3158, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=259, Total reward=77.02, Steps=3174, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=260, Total reward=52.01, Steps=3185, Training iteration=12
Policy training> Surrogate loss=0.03140678629279137, KL divergence=1.5189329133136198e-05, Entropy=1.8955631256103516, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.010702679865062237, KL divergence=0.00016658741515129805, Entropy=1.8949260711669922, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.017745167016983032, KL divergence=0.000517913606017828, Entropy=1.892858862876892, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.009362652897834778, KL divergence=0.001046728459186852, Entropy=1.8925701379776, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028317859396338463, KL divergence=0.0018966036150231957, Entropy=1.8910512924194336, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.0007548208232037723, KL divergence=0.0031845467165112495, Entropy=1.8874458074569702, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=0.04058576375246048, KL divergence=0.004886312410235405, Entropy=1.883859634399414, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0033437584061175585, KL divergence=0.007238615304231644, Entropy=1.8810373544692993, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.032671499997377396, KL divergence=0.010067458264529705, Entropy=1.8779696226119995, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.025212237611413002, KL divergence=0.012081601656973362, Entropy=1.8785358667373657, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/13_Step-3185.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/13_Step-3185.ckpt']
Uploaded 3 files for checkpoint 13 in 0.53 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_13.pb
Best checkpoint number: 4, Last checkpoint number: 11
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'10'}
Training> Name=main_level/agent, Worker=0, Episode=261, Total reward=47.01, Steps=3195, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=262, Total reward=52.01, Steps=3206, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=263, Total reward=53.01, Steps=3217, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=264, Total reward=152.03, Steps=3248, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=265, Total reward=82.02, Steps=3265, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=266, Total reward=52.01, Steps=3276, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=267, Total reward=112.03, Steps=3299, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=268, Total reward=88.02, Steps=3317, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=269, Total reward=53.01, Steps=3328, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=270, Total reward=37.01, Steps=3336, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=271, Total reward=76.02, Steps=3352, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=272, Total reward=43.01, Steps=3361, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=273, Total reward=68.02, Steps=3375, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=274, Total reward=63.02, Steps=3388, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=275, Total reward=71.02, Steps=3403, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=276, Total reward=37.01, Steps=3411, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=277, Total reward=52.01, Steps=3422, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=278, Total reward=78.02, Steps=3438, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=279, Total reward=42.01, Steps=3447, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=280, Total reward=62.02, Steps=3460, Training iteration=13
Policy training> Surrogate loss=-0.012370973825454712, KL divergence=7.010870467638597e-05, Entropy=1.8813059329986572, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.018742190673947334, KL divergence=0.0005218630540184677, Entropy=1.885695219039917, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.012999290600419044, KL divergence=0.0014647941570729017, Entropy=1.8867437839508057, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.010003222152590752, KL divergence=0.002782202558591962, Entropy=1.887187123298645, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.0005996134132146835, KL divergence=0.004351858515292406, Entropy=1.8860000371932983, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03703001141548157, KL divergence=0.005892252083867788, Entropy=1.885502576828003, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03848830610513687, KL divergence=0.006847020238637924, Entropy=1.8853669166564941, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.017700865864753723, KL divergence=0.007287505082786083, Entropy=1.8861935138702393, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.009700192138552666, KL divergence=0.007435916922986507, Entropy=1.8876043558120728, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02326657809317112, KL divergence=0.007389598526060581, Entropy=1.89064621925354, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/14_Step-3460.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/14_Step-3460.ckpt']
Uploaded 3 files for checkpoint 14 in 0.57 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_14.pb
Best checkpoint number: 4, Last checkpoint number: 12
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'11'}
Training> Name=main_level/agent, Worker=0, Episode=281, Total reward=86.02, Steps=3478, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=282, Total reward=68.02, Steps=3492, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=283, Total reward=47.01, Steps=3502, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=284, Total reward=43.01, Steps=3511, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=285, Total reward=82.02, Steps=3528, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=286, Total reward=52.01, Steps=3539, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=287, Total reward=43.01, Steps=3548, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=288, Total reward=92.02, Steps=3567, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=289, Total reward=42.01, Steps=3576, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=290, Total reward=43.01, Steps=3585, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=291, Total reward=76.02, Steps=3601, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=292, Total reward=61.02, Steps=3614, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=293, Total reward=107.03, Steps=3636, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=294, Total reward=126.03, Steps=3662, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=295, Total reward=75.02, Steps=3678, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=296, Total reward=83.02, Steps=3695, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=297, Total reward=121.04, Steps=3721, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=298, Total reward=77.02, Steps=3737, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=299, Total reward=47.01, Steps=3747, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=300, Total reward=51.02, Steps=3758, Training iteration=14
Policy training> Surrogate loss=0.035640910267829895, KL divergence=9.128689271165058e-06, Entropy=1.8923413753509521, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.005025885999202728, KL divergence=7.770012598484755e-05, Entropy=1.8953957557678223, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.03316124528646469, KL divergence=0.00028267764719203115, Entropy=1.898144245147705, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.017075590789318085, KL divergence=0.0006054361583665013, Entropy=1.8995507955551147, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.04592595994472504, KL divergence=0.0010707578621804714, Entropy=1.9015285968780518, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.021148791536688805, KL divergence=0.002129496308043599, Entropy=1.9019546508789062, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=0.006987582892179489, KL divergence=0.0037462497130036354, Entropy=1.9025993347167969, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0340595580637455, KL divergence=0.005615502130240202, Entropy=1.901848316192627, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.014271508902311325, KL divergence=0.0075031123124063015, Entropy=1.9008736610412598, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03454417362809181, KL divergence=0.008662646636366844, Entropy=1.9011973142623901, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/15_Step-3758.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/15_Step-3758.ckpt']
Uploaded 3 files for checkpoint 15 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_15.pb
Best checkpoint number: 4, Last checkpoint number: 13
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'12'}
Training> Name=main_level/agent, Worker=0, Episode=301, Total reward=118.03, Steps=3782, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=302, Total reward=57.02, Steps=3794, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=303, Total reward=51.02, Steps=3805, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=304, Total reward=38.01, Steps=3813, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=305, Total reward=47.01, Steps=3823, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=306, Total reward=38.01, Steps=3831, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=307, Total reward=72.02, Steps=3846, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=308, Total reward=94.03, Steps=3866, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=309, Total reward=43.01, Steps=3875, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=310, Total reward=140.03, Steps=3904, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=311, Total reward=62.02, Steps=3917, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=312, Total reward=90.02, Steps=3936, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=313, Total reward=139.04, Steps=3965, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=314, Total reward=38.01, Steps=3973, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=315, Total reward=43.01, Steps=3982, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=316, Total reward=32.01, Steps=3989, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=317, Total reward=68.02, Steps=4003, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=318, Total reward=83.02, Steps=4020, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=319, Total reward=38.01, Steps=4028, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=320, Total reward=73.02, Steps=4043, Training iteration=15
Policy training> Surrogate loss=0.014143761247396469, KL divergence=2.734681766014546e-05, Entropy=1.9036178588867188, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.0011962205171585083, KL divergence=0.00015443812299054116, Entropy=1.902719259262085, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.007228147238492966, KL divergence=0.0004069264105055481, Entropy=1.9018003940582275, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.006728632375597954, KL divergence=0.0008656785357743502, Entropy=1.9005613327026367, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.01966494880616665, KL divergence=0.0016132551245391369, Entropy=1.8978809118270874, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0214332714676857, KL divergence=0.0025985450483858585, Entropy=1.8949060440063477, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02065708115696907, KL divergence=0.003912449348717928, Entropy=1.8905704021453857, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.016416635364294052, KL divergence=0.0051219589076936245, Entropy=1.8878915309906006, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.009325601160526276, KL divergence=0.0065309121273458, Entropy=1.8855516910552979, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03945276141166687, KL divergence=0.007481026463210583, Entropy=1.8849210739135742, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/16_Step-4043.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/16_Step-4043.ckpt']
Uploaded 3 files for checkpoint 16 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_16.pb
Best checkpoint number: 4, Last checkpoint number: 14
Copying the frozen checkpoint from ./frozen_models/agent/model_4.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'13'}
Training> Name=main_level/agent, Worker=0, Episode=321, Total reward=57.02, Steps=4055, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=322, Total reward=32.01, Steps=4062, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=323, Total reward=38.01, Steps=4070, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=324, Total reward=43.01, Steps=4079, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=325, Total reward=67.02, Steps=4093, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=326, Total reward=38.01, Steps=4101, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=327, Total reward=72.02, Steps=4116, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=328, Total reward=52.01, Steps=4127, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=329, Total reward=67.02, Steps=4141, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=330, Total reward=68.02, Steps=4155, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=331, Total reward=67.02, Steps=4169, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=332, Total reward=62.02, Steps=4182, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=333, Total reward=68.02, Steps=4196, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=334, Total reward=57.02, Steps=4208, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=335, Total reward=52.01, Steps=4219, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=336, Total reward=97.02, Steps=4239, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=337, Total reward=71.02, Steps=4254, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=338, Total reward=56.02, Steps=4266, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=339, Total reward=37.01, Steps=4274, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=340, Total reward=35.01, Steps=4281, Training iteration=16
Policy training> Surrogate loss=-0.020989522337913513, KL divergence=4.069419446750544e-06, Entropy=1.8882497549057007, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.012995134107768536, KL divergence=3.5099696106044576e-05, Entropy=1.888471245765686, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.03312832489609718, KL divergence=0.0001442124048480764, Entropy=1.887144923210144, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.022459879517555237, KL divergence=0.0003975850122515112, Entropy=1.8876758813858032, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03829292207956314, KL divergence=0.0008979627746157348, Entropy=1.8871269226074219, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.006802864372730255, KL divergence=0.00174435053486377, Entropy=1.8856908082962036, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=0.0015966048231348395, KL divergence=0.0030716508626937866, Entropy=1.884291648864746, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029151858761906624, KL divergence=0.004939889069646597, Entropy=1.8825922012329102, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.006100068334490061, KL divergence=0.007492282893508673, Entropy=1.8798338174819946, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.007811088114976883, KL divergence=0.010679580271244049, Entropy=1.875783920288086, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/17_Step-4281.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/17_Step-4281.ckpt']
Uploaded 3 files for checkpoint 17 in 0.52 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_17.pb
Best checkpoint number: 15, Last checkpoint number: 15
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'14'}
Training> Name=main_level/agent, Worker=0, Episode=341, Total reward=55.01, Steps=4292, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=342, Total reward=163.04, Steps=4326, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=343, Total reward=47.01, Steps=4336, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=344, Total reward=47.01, Steps=4346, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=345, Total reward=58.01, Steps=4358, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=346, Total reward=48.01, Steps=4368, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=347, Total reward=77.02, Steps=4384, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=348, Total reward=47.01, Steps=4394, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=349, Total reward=47.01, Steps=4404, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=350, Total reward=112.03, Steps=4427, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=351, Total reward=56.02, Steps=4439, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=352, Total reward=47.01, Steps=4449, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=353, Total reward=48.01, Steps=4459, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=354, Total reward=38.01, Steps=4467, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=355, Total reward=38.01, Steps=4475, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=356, Total reward=127.03, Steps=4501, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=357, Total reward=53.01, Steps=4512, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=358, Total reward=78.02, Steps=4528, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=359, Total reward=43.01, Steps=4537, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=360, Total reward=102.02, Steps=4558, Training iteration=17
Policy training> Surrogate loss=0.003497440367937088, KL divergence=4.3342621211195365e-05, Entropy=1.8769285678863525, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.01216692291200161, KL divergence=0.00036844349233433604, Entropy=1.8759602308273315, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.003817496821284294, KL divergence=0.0012772604823112488, Entropy=1.8751615285873413, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.016989637166261673, KL divergence=0.002583499997854233, Entropy=1.8731443881988525, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.017499614506959915, KL divergence=0.004171046428382397, Entropy=1.8721580505371094, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.01507557276636362, KL divergence=0.0069367364048957825, Entropy=1.8687678575515747, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03876610845327377, KL divergence=0.01041860319674015, Entropy=1.8632944822311401, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.011367715895175934, KL divergence=0.012162049300968647, Entropy=1.8589434623718262, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.01461873110383749, KL divergence=0.0115914735943079, Entropy=1.8592132329940796, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.016509635373950005, KL divergence=0.01178150437772274, Entropy=1.8586292266845703, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/18_Step-4558.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/18_Step-4558.ckpt']
Uploaded 3 files for checkpoint 18 in 0.52 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_18.pb
Best checkpoint number: 15, Last checkpoint number: 16
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'4'}
Training> Name=main_level/agent, Worker=0, Episode=361, Total reward=56.02, Steps=4570, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=362, Total reward=77.02, Steps=4586, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=363, Total reward=72.02, Steps=4601, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=364, Total reward=42.01, Steps=4610, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=365, Total reward=42.01, Steps=4619, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=366, Total reward=33.01, Steps=4626, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=367, Total reward=137.03, Steps=4654, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=368, Total reward=47.01, Steps=4664, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=369, Total reward=82.02, Steps=4681, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=370, Total reward=57.02, Steps=4693, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=371, Total reward=62.02, Steps=4706, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=372, Total reward=48.01, Steps=4716, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=373, Total reward=95.03, Steps=4736, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=374, Total reward=66.02, Steps=4750, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=375, Total reward=44.01, Steps=4759, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=376, Total reward=88.02, Steps=4777, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=377, Total reward=53.01, Steps=4788, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=378, Total reward=117.03, Steps=4812, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=379, Total reward=38.01, Steps=4820, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=380, Total reward=117.03, Steps=4844, Training iteration=18
Policy training> Surrogate loss=-0.008745770901441574, KL divergence=5.72557519262773e-06, Entropy=1.857142448425293, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.0075853317975997925, KL divergence=0.0001210060145240277, Entropy=1.8557766675949097, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.012380354106426239, KL divergence=0.0007025165250524879, Entropy=1.8497356176376343, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.03762476146221161, KL divergence=0.002195352455601096, Entropy=1.845344066619873, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0119369737803936, KL divergence=0.005553886294364929, Entropy=1.8370589017868042, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0008684112690389156, KL divergence=0.010608673095703125, Entropy=1.8260045051574707, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=0.011663194745779037, KL divergence=0.014011131599545479, Entropy=1.8199374675750732, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.04941973835229874, KL divergence=0.013229383155703545, Entropy=1.8286831378936768, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03957337513566017, KL divergence=0.011831678450107574, Entropy=1.8384898900985718, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.024912720546126366, KL divergence=0.012467592023313046, Entropy=1.8386759757995605, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/19_Step-4844.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/19_Step-4844.ckpt']
Uploaded 3 files for checkpoint 19 in 0.56 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_19.pb
Best checkpoint number: 15, Last checkpoint number: 17
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'16'}
Training> Name=main_level/agent, Worker=0, Episode=381, Total reward=33.01, Steps=4851, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=382, Total reward=52.01, Steps=4862, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=383, Total reward=81.02, Steps=4879, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=384, Total reward=73.02, Steps=4894, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=385, Total reward=73.02, Steps=4909, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=386, Total reward=38.01, Steps=4917, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=387, Total reward=48.01, Steps=4927, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=388, Total reward=178.04, Steps=4963, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=389, Total reward=82.02, Steps=4980, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=390, Total reward=75.02, Steps=4995, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=391, Total reward=22.01, Steps=5000, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=392, Total reward=82.02, Steps=5017, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=393, Total reward=68.02, Steps=5031, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=394, Total reward=52.01, Steps=5042, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=395, Total reward=38.01, Steps=5050, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=396, Total reward=58.01, Steps=5062, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=397, Total reward=57.02, Steps=5074, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=398, Total reward=113.03, Steps=5097, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=399, Total reward=132.03, Steps=5124, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=400, Total reward=37.01, Steps=5132, Training iteration=19
Policy training> Surrogate loss=0.017472337931394577, KL divergence=7.248762995004654e-05, Entropy=1.8388866186141968, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.006058282218873501, KL divergence=0.0002553280210122466, Entropy=1.8347525596618652, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.029182851314544678, KL divergence=0.0004330373485572636, Entropy=1.838244915008545, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.012982672080397606, KL divergence=0.0007855519652366638, Entropy=1.8421587944030762, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0020607393234968185, KL divergence=0.0015656835166737437, Entropy=1.8470423221588135, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.029484190046787262, KL divergence=0.002811938989907503, Entropy=1.847333312034607, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.013502202928066254, KL divergence=0.004461538977921009, Entropy=1.8498098850250244, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.016584444791078568, KL divergence=0.006334098055958748, Entropy=1.8568320274353027, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=0.013845458626747131, KL divergence=0.008476928807795048, Entropy=1.86241614818573, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.014416206628084183, KL divergence=0.010055894032120705, Entropy=1.8659518957138062, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/20_Step-5132.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/20_Step-5132.ckpt']
Uploaded 3 files for checkpoint 20 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_20.pb
Best checkpoint number: 15, Last checkpoint number: 18
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'17'}
Training> Name=main_level/agent, Worker=0, Episode=401, Total reward=60.01, Steps=5144, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=402, Total reward=78.02, Steps=5160, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=403, Total reward=68.02, Steps=5174, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=404, Total reward=107.03, Steps=5196, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=405, Total reward=73.02, Steps=5211, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=406, Total reward=37.01, Steps=5219, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=407, Total reward=58.01, Steps=5231, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=408, Total reward=92.02, Steps=5250, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=409, Total reward=62.02, Steps=5263, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=410, Total reward=90.02, Steps=5281, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=411, Total reward=128.03, Steps=5307, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=412, Total reward=47.01, Steps=5317, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=413, Total reward=74.02, Steps=5333, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=414, Total reward=77.02, Steps=5350, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=415, Total reward=47.01, Steps=5360, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=416, Total reward=52.01, Steps=5371, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=417, Total reward=47.01, Steps=5381, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=418, Total reward=52.01, Steps=5392, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=419, Total reward=47.01, Steps=5402, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=420, Total reward=78.02, Steps=5418, Training iteration=20
Policy training> Surrogate loss=0.030890215188264847, KL divergence=7.571273272333201e-06, Entropy=1.8670783042907715, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.029266290366649628, KL divergence=4.935587276122533e-05, Entropy=1.869156837463379, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.010434620082378387, KL divergence=0.0002615689008962363, Entropy=1.874918818473816, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.03206759691238403, KL divergence=0.0007600482786074281, Entropy=1.8792426586151123, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.005006667226552963, KL divergence=0.001990789780393243, Entropy=1.8862500190734863, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.001741679385304451, KL divergence=0.0040049441158771515, Entropy=1.8936187028884888, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.039102181792259216, KL divergence=0.006518475711345673, Entropy=1.8991427421569824, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.004735349677503109, KL divergence=0.00922374613583088, Entropy=1.9031230211257935, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.00762394443154335, KL divergence=0.011169292964041233, Entropy=1.903930425643921, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.009629025124013424, KL divergence=0.011666283011436462, Entropy=1.9025933742523193, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/21_Step-5418.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/21_Step-5418.ckpt']
Uploaded 3 files for checkpoint 21 in 0.49 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_21.pb
Best checkpoint number: 15, Last checkpoint number: 19
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'18'}
Training> Name=main_level/agent, Worker=0, Episode=421, Total reward=87.02, Steps=5436, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=422, Total reward=46.01, Steps=5446, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=423, Total reward=118.03, Steps=5471, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=424, Total reward=63.02, Steps=5484, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=425, Total reward=71.02, Steps=5499, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=426, Total reward=37.01, Steps=5507, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=427, Total reward=113.03, Steps=5531, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=428, Total reward=37.01, Steps=5539, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=429, Total reward=88.02, Steps=5557, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=430, Total reward=97.02, Steps=5577, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=431, Total reward=87.02, Steps=5595, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=432, Total reward=57.02, Steps=5607, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=433, Total reward=179.05, Steps=5645, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=434, Total reward=60.02, Steps=5658, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=435, Total reward=121.03, Steps=5683, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=436, Total reward=42.01, Steps=5692, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=437, Total reward=37.01, Steps=5700, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=438, Total reward=90.02, Steps=5719, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=439, Total reward=46.01, Steps=5729, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=440, Total reward=72.02, Steps=5744, Training iteration=21
Policy training> Surrogate loss=-0.006872082594782114, KL divergence=4.7090543375816196e-05, Entropy=1.8952043056488037, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.007718653883785009, KL divergence=0.0006683209794573486, Entropy=1.8859943151474, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.021405111998319626, KL divergence=0.002863029483705759, Entropy=1.8706079721450806, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.013375240378081799, KL divergence=0.007209498435258865, Entropy=1.8524919748306274, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.01284153014421463, KL divergence=0.009700236842036247, Entropy=1.845238447189331, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.032568808645009995, KL divergence=0.008060113526880741, Entropy=1.8546066284179688, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.011641273275017738, KL divergence=0.007449687458574772, Entropy=1.8591992855072021, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.014239238575100899, KL divergence=0.008242445066571236, Entropy=1.858188271522522, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.023777982220053673, KL divergence=0.010373474098742008, Entropy=1.8501789569854736, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.029600108042359352, KL divergence=0.01061209850013256, Entropy=1.848761796951294, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/22_Step-5744.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/22_Step-5744.ckpt']
Uploaded 3 files for checkpoint 22 in 0.53 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_22.pb
Best checkpoint number: 15, Last checkpoint number: 20
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'19'}
Training> Name=main_level/agent, Worker=0, Episode=441, Total reward=42.01, Steps=5753, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=442, Total reward=62.02, Steps=5766, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=443, Total reward=33.01, Steps=5773, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=444, Total reward=42.01, Steps=5782, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=445, Total reward=57.02, Steps=5794, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=446, Total reward=33.01, Steps=5801, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=447, Total reward=48.01, Steps=5811, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=448, Total reward=38.01, Steps=5819, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=449, Total reward=67.02, Steps=5833, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=450, Total reward=42.01, Steps=5842, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=451, Total reward=53.01, Steps=5853, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=452, Total reward=47.01, Steps=5863, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=453, Total reward=128.03, Steps=5889, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=454, Total reward=38.01, Steps=5897, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=455, Total reward=123.03, Steps=5923, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=456, Total reward=73.02, Steps=5938, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=457, Total reward=33.01, Steps=5945, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=458, Total reward=62.02, Steps=5958, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=459, Total reward=91.02, Steps=5977, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=460, Total reward=46.01, Steps=5987, Training iteration=22
Policy training> Surrogate loss=-0.057841259986162186, KL divergence=1.1521003216330428e-05, Entropy=1.849389672279358, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.06410084664821625, KL divergence=0.00010303223825758323, Entropy=1.848414421081543, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.010265205055475235, KL divergence=0.0003971554397139698, Entropy=1.8449339866638184, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.007730805780738592, KL divergence=0.0012304886477068067, Entropy=1.8363847732543945, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.016278689727187157, KL divergence=0.002534066326916218, Entropy=1.828187108039856, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.004711417946964502, KL divergence=0.004303773399442434, Entropy=1.8200184106826782, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03429355099797249, KL divergence=0.00544331781566143, Entropy=1.8190950155258179, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=0.030327623710036278, KL divergence=0.006400551181286573, Entropy=1.815829873085022, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=0.030420979484915733, KL divergence=0.006880205124616623, Entropy=1.814615249633789, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.04015485569834709, KL divergence=0.007137646432965994, Entropy=1.8126717805862427, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/23_Step-5987.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/23_Step-5987.ckpt']
Uploaded 3 files for checkpoint 23 in 0.55 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_23.pb
Best checkpoint number: 15, Last checkpoint number: 21
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'20'}
Training> Name=main_level/agent, Worker=0, Episode=461, Total reward=38.01, Steps=5995, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=462, Total reward=100.03, Steps=6016, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=463, Total reward=43.01, Steps=6025, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=464, Total reward=83.02, Steps=6042, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=465, Total reward=52.01, Steps=6053, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=466, Total reward=38.01, Steps=6061, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=467, Total reward=47.01, Steps=6071, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=468, Total reward=207.05, Steps=6113, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=469, Total reward=62.02, Steps=6126, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=470, Total reward=106.03, Steps=6148, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=471, Total reward=107.03, Steps=6170, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=472, Total reward=47.01, Steps=6180, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=473, Total reward=67.02, Steps=6194, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=474, Total reward=112.03, Steps=6217, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=475, Total reward=38.01, Steps=6225, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=476, Total reward=73.02, Steps=6240, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=477, Total reward=127.03, Steps=6266, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=478, Total reward=127.03, Steps=6292, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=479, Total reward=83.02, Steps=6309, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=480, Total reward=77.02, Steps=6325, Training iteration=23
Policy training> Surrogate loss=-0.011613341979682446, KL divergence=6.099063830333762e-05, Entropy=1.8229188919067383, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.007708651479333639, KL divergence=0.000542060355655849, Entropy=1.8291738033294678, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.0017904043197631836, KL divergence=0.0018785916035994887, Entropy=1.840936303138733, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=7.123649265849963e-05, KL divergence=0.004485623445361853, Entropy=1.8487800359725952, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.024249020963907242, KL divergence=0.009763713926076889, Entropy=1.8610035181045532, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.014572301879525185, KL divergence=0.013443462550640106, Entropy=1.8657424449920654, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.008946962654590607, KL divergence=0.011542529799044132, Entropy=1.8564485311508179, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.012165580876171589, KL divergence=0.010698193684220314, Entropy=1.8502353429794312, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.028359070420265198, KL divergence=0.011040190234780312, Entropy=1.850680947303772, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02104993537068367, KL divergence=0.010219598188996315, Entropy=1.8543727397918701, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/24_Step-6325.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/24_Step-6325.ckpt']
Uploaded 3 files for checkpoint 24 in 0.55 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_24.pb
Best checkpoint number: 15, Last checkpoint number: 22
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'21'}
Training> Name=main_level/agent, Worker=0, Episode=481, Total reward=43.01, Steps=6334, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=482, Total reward=58.01, Steps=6346, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=483, Total reward=56.02, Steps=6358, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=484, Total reward=102.02, Steps=6379, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=485, Total reward=83.02, Steps=6396, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=486, Total reward=53.01, Steps=6407, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=487, Total reward=67.02, Steps=6421, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=488, Total reward=43.01, Steps=6430, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=489, Total reward=42.01, Steps=6439, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=490, Total reward=62.02, Steps=6452, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=491, Total reward=30.01, Steps=6458, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=492, Total reward=57.02, Steps=6470, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=493, Total reward=78.02, Steps=6486, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=494, Total reward=71.02, Steps=6501, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=495, Total reward=38.01, Steps=6509, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=496, Total reward=117.03, Steps=6533, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=497, Total reward=60.02, Steps=6546, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=498, Total reward=88.02, Steps=6564, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=499, Total reward=88.02, Steps=6582, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=500, Total reward=63.02, Steps=6595, Training iteration=24
Policy training> Surrogate loss=-0.0007511153817176819, KL divergence=1.5028591406007763e-05, Entropy=1.854280710220337, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015078254044055939, KL divergence=0.0006160608609206975, Entropy=1.8429083824157715, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.005049554631114006, KL divergence=0.0037037075962871313, Entropy=1.8208255767822266, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023278435692191124, KL divergence=0.010052280500531197, Entropy=1.7916815280914307, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03358272463083267, KL divergence=0.012384837493300438, Entropy=1.7838720083236694, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02425346150994301, KL divergence=0.008887412957847118, Entropy=1.7971628904342651, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.031560804694890976, KL divergence=0.0064284601248800755, Entropy=1.8083314895629883, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.017385432496666908, KL divergence=0.008228171616792679, Entropy=1.8025790452957153, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.009838555008172989, KL divergence=0.010589753277599812, Entropy=1.792153239250183, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.016815558075904846, KL divergence=0.010186152532696724, Entropy=1.7968792915344238, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/25_Step-6595.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/25_Step-6595.ckpt']
Uploaded 3 files for checkpoint 25 in 0.58 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_25.pb
Best checkpoint number: 15, Last checkpoint number: 23
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'22'}
Training> Name=main_level/agent, Worker=0, Episode=501, Total reward=37.01, Steps=6603, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=502, Total reward=117.03, Steps=6627, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=503, Total reward=91.02, Steps=6646, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=504, Total reward=138.04, Steps=6675, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=505, Total reward=43.01, Steps=6684, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=506, Total reward=67.02, Steps=6698, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=507, Total reward=43.01, Steps=6707, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=508, Total reward=47.01, Steps=6717, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=509, Total reward=52.01, Steps=6728, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=510, Total reward=68.02, Steps=6743, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=511, Total reward=100.03, Steps=6765, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=512, Total reward=47.01, Steps=6775, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=513, Total reward=127.03, Steps=6801, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=514, Total reward=148.03, Steps=6831, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=515, Total reward=42.01, Steps=6840, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=516, Total reward=83.02, Steps=6857, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=517, Total reward=28.01, Steps=6863, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=518, Total reward=105.02, Steps=6884, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=519, Total reward=83.02, Steps=6901, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=520, Total reward=35.01, Steps=6908, Training iteration=25
Policy training> Surrogate loss=0.011671595275402069, KL divergence=4.88816003780812e-05, Entropy=1.8018736839294434, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.0036694519221782684, KL divergence=0.00028997822664678097, Entropy=1.7990832328796387, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.033895209431648254, KL divergence=0.0008289659162983298, Entropy=1.7953991889953613, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.006871182471513748, KL divergence=0.0020435266196727753, Entropy=1.7827143669128418, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.0023581497371196747, KL divergence=0.003696026047691703, Entropy=1.7766093015670776, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.0491003543138504, KL divergence=0.005719730164855719, Entropy=1.763946771621704, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.05649369955062866, KL divergence=0.006627364084124565, Entropy=1.7647682428359985, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=0.0065812841057777405, KL divergence=0.0069010332226753235, Entropy=1.77451753616333, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03922271355986595, KL divergence=0.00705233309417963, Entropy=1.7839837074279785, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.07086409628391266, KL divergence=0.007315339054912329, Entropy=1.792779564857483, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/26_Step-6908.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/26_Step-6908.ckpt']
Uploaded 3 files for checkpoint 26 in 0.65 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_26.pb
Best checkpoint number: 15, Last checkpoint number: 24
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'23'}
Training> Name=main_level/agent, Worker=0, Episode=521, Total reward=43.01, Steps=6917, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=522, Total reward=48.01, Steps=6927, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=523, Total reward=127.03, Steps=6953, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=524, Total reward=57.02, Steps=6965, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=525, Total reward=97.02, Steps=6985, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=526, Total reward=42.01, Steps=6994, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=527, Total reward=38.01, Steps=7002, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=528, Total reward=47.01, Steps=7012, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=529, Total reward=53.01, Steps=7023, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=530, Total reward=70.01, Steps=7037, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=531, Total reward=46.01, Steps=7047, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=532, Total reward=93.02, Steps=7066, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=533, Total reward=88.02, Steps=7084, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=534, Total reward=63.02, Steps=7098, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=535, Total reward=42.01, Steps=7107, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=536, Total reward=73.02, Steps=7122, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=537, Total reward=118.03, Steps=7146, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=538, Total reward=77.02, Steps=7162, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=539, Total reward=95.02, Steps=7181, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=540, Total reward=115.02, Steps=7204, Training iteration=26
Policy training> Surrogate loss=0.00398984644562006, KL divergence=7.048962288536131e-05, Entropy=1.807403564453125, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.021888870745897293, KL divergence=0.0011490427423268557, Entropy=1.8110759258270264, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02641313709318638, KL divergence=0.005121592432260513, Entropy=1.8127715587615967, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.014255281537771225, KL divergence=0.011930448934435844, Entropy=1.8145389556884766, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.015892695635557175, KL divergence=0.01573874056339264, Entropy=1.816202998161316, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.015525339171290398, KL divergence=0.014416560530662537, Entropy=1.8149261474609375, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.05048147588968277, KL divergence=0.011804915964603424, Entropy=1.810223937034607, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.024631906300783157, KL divergence=0.010844700038433075, Entropy=1.798771619796753, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.014232072979211807, KL divergence=0.011223975569009781, Entropy=1.7921648025512695, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.032736871391534805, KL divergence=0.009962193667888641, Entropy=1.7888060808181763, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/27_Step-7204.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/27_Step-7204.ckpt']
Uploaded 3 files for checkpoint 27 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_27.pb
Best checkpoint number: 15, Last checkpoint number: 25
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'24'}
Training> Name=main_level/agent, Worker=0, Episode=541, Total reward=83.02, Steps=7221, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=542, Total reward=48.01, Steps=7231, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=543, Total reward=98.02, Steps=7251, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=544, Total reward=103.02, Steps=7272, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=545, Total reward=102.02, Steps=7293, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=546, Total reward=72.02, Steps=7308, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=547, Total reward=43.01, Steps=7317, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=548, Total reward=52.01, Steps=7328, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=549, Total reward=62.02, Steps=7341, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=550, Total reward=59.01, Steps=7353, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=551, Total reward=27.01, Steps=7359, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=552, Total reward=108.02, Steps=7381, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=553, Total reward=71.02, Steps=7396, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=554, Total reward=82.02, Steps=7413, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=555, Total reward=72.02, Steps=7428, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=556, Total reward=68.02, Steps=7442, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=557, Total reward=51.02, Steps=7453, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=558, Total reward=116.03, Steps=7477, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=559, Total reward=110.02, Steps=7499, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=560, Total reward=97.02, Steps=7519, Training iteration=27
Policy training> Surrogate loss=-0.03670000284910202, KL divergence=0.00012398738181218505, Entropy=1.7908568382263184, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017411842942237854, KL divergence=0.0012508484069257975, Entropy=1.789634108543396, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.002908773720264435, KL divergence=0.003674214007332921, Entropy=1.7771764993667603, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.05301408842206001, KL divergence=0.004930301569402218, Entropy=1.7730960845947266, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.005693692713975906, KL divergence=0.005332573316991329, Entropy=1.7621735334396362, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.027734676375985146, KL divergence=0.0052718352526426315, Entropy=1.7641143798828125, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.07903125137090683, KL divergence=0.005687455181032419, Entropy=1.7705223560333252, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.048484236001968384, KL divergence=0.0075620319694280624, Entropy=1.7777559757232666, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.06185512989759445, KL divergence=0.008353650569915771, Entropy=1.7721428871154785, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.05110221356153488, KL divergence=0.007011111360043287, Entropy=1.773136019706726, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/28_Step-7519.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/28_Step-7519.ckpt']
Uploaded 3 files for checkpoint 28 in 0.53 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_28.pb
Best checkpoint number: 15, Last checkpoint number: 26
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'25'}
Training> Name=main_level/agent, Worker=0, Episode=561, Total reward=38.01, Steps=7527, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=562, Total reward=62.02, Steps=7540, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=563, Total reward=96.02, Steps=7560, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=564, Total reward=46.01, Steps=7570, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=565, Total reward=98.02, Steps=7590, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=566, Total reward=58.01, Steps=7602, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=567, Total reward=58.01, Steps=7614, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=568, Total reward=81.02, Steps=7631, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=569, Total reward=113.03, Steps=7654, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=570, Total reward=77.02, Steps=7670, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=571, Total reward=42.01, Steps=7679, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=572, Total reward=52.01, Steps=7690, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=573, Total reward=88.02, Steps=7708, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=574, Total reward=38.01, Steps=7716, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=575, Total reward=92.02, Steps=7735, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=576, Total reward=42.01, Steps=7744, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=577, Total reward=52.01, Steps=7755, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=578, Total reward=133.03, Steps=7782, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=579, Total reward=73.02, Steps=7797, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=580, Total reward=50.01, Steps=7807, Training iteration=28
Policy training> Surrogate loss=0.01908978633582592, KL divergence=7.191189797595143e-05, Entropy=1.7644257545471191, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014490842819213867, KL divergence=0.0015116716967895627, Entropy=1.7689223289489746, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.029593028128147125, KL divergence=0.009216820821166039, Entropy=1.7574552297592163, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025739239528775215, KL divergence=0.018259674310684204, Entropy=1.7391996383666992, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.0005213003605604172, KL divergence=0.01684066280722618, Entropy=1.7317795753479004, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.024458853527903557, KL divergence=0.009653734974563122, Entropy=1.7366478443145752, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.04354378208518028, KL divergence=0.008741537109017372, Entropy=1.7364699840545654, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=0.019503727555274963, KL divergence=0.010695231147110462, Entropy=1.7395631074905396, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.030904967337846756, KL divergence=0.012345924973487854, Entropy=1.7243419885635376, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.01727036014199257, KL divergence=0.009436849504709244, Entropy=1.7250308990478516, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/29_Step-7807.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/29_Step-7807.ckpt']
Uploaded 3 files for checkpoint 29 in 0.52 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_29.pb
Best checkpoint number: 15, Last checkpoint number: 27
Copying the frozen checkpoint from ./frozen_models/agent/model_15.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'26'}
Training> Name=main_level/agent, Worker=0, Episode=581, Total reward=138.03, Steps=7835, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=582, Total reward=98.02, Steps=7855, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=583, Total reward=67.02, Steps=7869, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=584, Total reward=83.02, Steps=7886, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=585, Total reward=78.02, Steps=7903, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=586, Total reward=43.01, Steps=7912, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=587, Total reward=33.01, Steps=7919, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=588, Total reward=69.02, Steps=7934, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=589, Total reward=116.03, Steps=7958, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=590, Total reward=48.01, Steps=7968, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=591, Total reward=114.02, Steps=7991, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=592, Total reward=57.02, Steps=8003, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=593, Total reward=138.03, Steps=8031, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=594, Total reward=83.02, Steps=8048, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=595, Total reward=43.01, Steps=8057, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=596, Total reward=37.01, Steps=8065, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=597, Total reward=117.03, Steps=8089, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=598, Total reward=118.03, Steps=8113, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=599, Total reward=77.02, Steps=8129, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=600, Total reward=56.02, Steps=8141, Training iteration=29
Policy training> Surrogate loss=0.01638067699968815, KL divergence=0.0005459369858726859, Entropy=1.728912591934204, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.0034586130641400814, KL divergence=0.006114358548074961, Entropy=1.7057863473892212, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.024631474167108536, KL divergence=0.013427731581032276, Entropy=1.6883041858673096, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02365501970052719, KL divergence=0.012717822566628456, Entropy=1.6945816278457642, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028097212314605713, KL divergence=0.009270635433495045, Entropy=1.6970641613006592, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0144215552136302, KL divergence=0.011521590873599052, Entropy=1.6964839696884155, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.04597775265574455, KL divergence=0.013595573604106903, Entropy=1.6920112371444702, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.018014680594205856, KL divergence=0.011888121254742146, Entropy=1.6907354593276978, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.0330313965678215, KL divergence=0.010411322116851807, Entropy=1.6904518604278564, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.06185917928814888, KL divergence=0.01186954416334629, Entropy=1.686762809753418, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/30_Step-8141.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/30_Step-8141.ckpt']
Uploaded 3 files for checkpoint 30 in 0.52 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_30.pb
Best checkpoint number: 28, Last checkpoint number: 28
Copying the frozen checkpoint from ./frozen_models/agent/model_28.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'15'}
Training> Name=main_level/agent, Worker=0, Episode=601, Total reward=47.01, Steps=8151, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=602, Total reward=53.01, Steps=8162, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=603, Total reward=62.02, Steps=8175, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=604, Total reward=62.02, Steps=8188, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=605, Total reward=43.01, Steps=8197, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=606, Total reward=42.01, Steps=8206, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=607, Total reward=37.01, Steps=8214, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=608, Total reward=42.01, Steps=8223, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=609, Total reward=77.02, Steps=8239, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=610, Total reward=48.01, Steps=8249, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=611, Total reward=87.02, Steps=8267, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=612, Total reward=294.07, Steps=8327, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=613, Total reward=72.02, Steps=8342, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=614, Total reward=53.01, Steps=8353, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=615, Total reward=43.01, Steps=8362, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=616, Total reward=42.01, Steps=8371, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=617, Total reward=38.01, Steps=8379, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=618, Total reward=78.02, Steps=8395, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=619, Total reward=56.02, Steps=8407, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=620, Total reward=35.01, Steps=8414, Training iteration=30
Policy training> Surrogate loss=-0.015414983034133911, KL divergence=0.0002643715124577284, Entropy=1.6866321563720703, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014436706900596619, KL divergence=0.005331770516932011, Entropy=1.674843192100525, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.018859779462218285, KL divergence=0.020299866795539856, Entropy=1.6520302295684814, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.014107340946793556, KL divergence=0.02081270143389702, Entropy=1.6468373537063599, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.013377487659454346, KL divergence=0.014983836561441422, Entropy=1.6593258380889893, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03319769725203514, KL divergence=0.013603800907731056, Entropy=1.6616160869598389, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.047046005725860596, KL divergence=0.010081359185278416, Entropy=1.6659636497497559, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.050527866929769516, KL divergence=0.007899260148406029, Entropy=1.6630311012268066, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.030609101057052612, KL divergence=0.011074801906943321, Entropy=1.6493265628814697, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.036028873175382614, KL divergence=0.011949676088988781, Entropy=1.6626745462417603, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/31_Step-8414.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/31_Step-8414.ckpt']
Uploaded 3 files for checkpoint 31 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_31.pb
Best checkpoint number: 28, Last checkpoint number: 29
Copying the frozen checkpoint from ./frozen_models/agent/model_28.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'27'}
Training> Name=main_level/agent, Worker=0, Episode=621, Total reward=80.02, Steps=8430, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=622, Total reward=52.01, Steps=8441, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=623, Total reward=67.02, Steps=8455, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=624, Total reward=38.01, Steps=8463, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=625, Total reward=261.06, Steps=8516, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=626, Total reward=42.01, Steps=8525, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=627, Total reward=112.03, Steps=8548, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=628, Total reward=170.03, Steps=8582, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=629, Total reward=97.02, Steps=8602, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=630, Total reward=62.02, Steps=8615, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=631, Total reward=25.01, Steps=8620, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=632, Total reward=126.03, Steps=8646, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=633, Total reward=77.02, Steps=8662, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=634, Total reward=112.03, Steps=8685, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=635, Total reward=92.02, Steps=8704, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=636, Total reward=42.01, Steps=8713, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=637, Total reward=160.04, Steps=8746, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=638, Total reward=78.02, Steps=8762, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=639, Total reward=188.04, Steps=8800, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=640, Total reward=132.03, Steps=8827, Training iteration=31
Policy training> Surrogate loss=0.010267592966556549, KL divergence=0.0006790328188799322, Entropy=1.6761053800582886, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.004833004903048277, KL divergence=0.004394031595438719, Entropy=1.6416040658950806, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.005550418049097061, KL divergence=0.009492960758507252, Entropy=1.6052502393722534, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.016642430797219276, KL divergence=0.007801948115229607, Entropy=1.6039530038833618, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.012653476558625698, KL divergence=0.005381198134273291, Entropy=1.6157232522964478, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.021943723782896996, KL divergence=0.005947239231318235, Entropy=1.6197214126586914, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.013747595250606537, KL divergence=0.008343435823917389, Entropy=1.6032379865646362, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.01020167674869299, KL divergence=0.006633374840021133, Entropy=1.615403652191162, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.0446045845746994, KL divergence=0.005413986276835203, Entropy=1.6269659996032715, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.01818358711898327, KL divergence=0.00937635451555252, Entropy=1.595033049583435, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/32_Step-8827.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/32_Step-8827.ckpt']
Uploaded 3 files for checkpoint 32 in 0.54 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_32.pb
Best checkpoint number: 30, Last checkpoint number: 30
Copying the frozen checkpoint from ./frozen_models/agent/model_30.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'28'}
Training> Name=main_level/agent, Worker=0, Episode=641, Total reward=56.02, Steps=8839, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=642, Total reward=117.03, Steps=8863, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=643, Total reward=42.01, Steps=8872, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=644, Total reward=142.03, Steps=8901, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=645, Total reward=42.01, Steps=8910, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=646, Total reward=38.01, Steps=8918, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=647, Total reward=103.02, Steps=8939, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=648, Total reward=72.02, Steps=8954, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=649, Total reward=107.03, Steps=8976, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=650, Total reward=92.02, Steps=8995, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=651, Total reward=167.04, Steps=9029, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=652, Total reward=242.05, Steps=9078, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=653, Total reward=102.02, Steps=9099, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=654, Total reward=72.02, Steps=9114, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=655, Total reward=131.03, Steps=9141, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=656, Total reward=133.03, Steps=9168, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=657, Total reward=102.02, Steps=9189, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=658, Total reward=103.02, Steps=9210, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=659, Total reward=105.03, Steps=9232, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=660, Total reward=78.02, Steps=9248, Training iteration=32
Policy training> Surrogate loss=-0.009769923985004425, KL divergence=0.00017842925444711, Entropy=1.5718380212783813, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015148021280765533, KL divergence=0.006451199296861887, Entropy=1.552822470664978, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.009297092445194721, KL divergence=0.010989521630108356, Entropy=1.563890814781189, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.018771803006529808, KL divergence=0.009428612887859344, Entropy=1.5649100542068481, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.035681262612342834, KL divergence=0.008735865354537964, Entropy=1.5315604209899902, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.036916449666023254, KL divergence=0.01299059484153986, Entropy=1.533685564994812, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03427482023835182, KL divergence=0.01032490748912096, Entropy=1.557198166847229, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.025980576872825623, KL divergence=0.010448201559484005, Entropy=1.5269851684570312, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.049659669399261475, KL divergence=0.009709224104881287, Entropy=1.5230165719985962, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.013362673111259937, KL divergence=0.005879890639334917, Entropy=1.5654281377792358, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/33_Step-9248.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/33_Step-9248.ckpt']
Uploaded 3 files for checkpoint 33 in 0.54 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_33.pb
Best checkpoint number: 30, Last checkpoint number: 31
Copying the frozen checkpoint from ./frozen_models/agent/model_30.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'29'}
Training> Name=main_level/agent, Worker=0, Episode=661, Total reward=47.01, Steps=9258, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=662, Total reward=107.03, Steps=9280, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=663, Total reward=112.03, Steps=9303, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=664, Total reward=72.02, Steps=9318, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=665, Total reward=89.02, Steps=9336, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=666, Total reward=49.01, Steps=9346, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=667, Total reward=93.02, Steps=9365, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=668, Total reward=53.01, Steps=9376, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=669, Total reward=147.03, Steps=9406, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=670, Total reward=80.02, Steps=9422, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=671, Total reward=93.03, Steps=9442, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=672, Total reward=121.03, Steps=9467, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=673, Total reward=122.03, Steps=9492, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=674, Total reward=82.02, Steps=9509, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=675, Total reward=47.01, Steps=9519, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=676, Total reward=118.03, Steps=9543, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=677, Total reward=301.06, Steps=9604, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=678, Total reward=47.01, Steps=9614, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=679, Total reward=102.02, Steps=9635, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=680, Total reward=35.01, Steps=9642, Training iteration=33
Policy training> Surrogate loss=0.004093378782272339, KL divergence=0.0021477066911756992, Entropy=1.5637086629867554, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.018594583496451378, KL divergence=0.01237381249666214, Entropy=1.5446797609329224, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.013771075755357742, KL divergence=0.01496288925409317, Entropy=1.5391815900802612, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.01740390993654728, KL divergence=0.008655744604766369, Entropy=1.5483293533325195, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.033602405339479446, KL divergence=0.007617237512022257, Entropy=1.5519994497299194, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03933747112751007, KL divergence=0.007376345340162516, Entropy=1.5434370040893555, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.038711193948984146, KL divergence=0.008727378211915493, Entropy=1.5453089475631714, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.04119016230106354, KL divergence=0.0066631268709897995, Entropy=1.5369406938552856, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03914831206202507, KL divergence=0.00818209070712328, Entropy=1.5391950607299805, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.031300708651542664, KL divergence=0.007941164076328278, Entropy=1.5423645973205566, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/34_Step-9642.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/34_Step-9642.ckpt']
Uploaded 3 files for checkpoint 34 in 0.48 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_34.pb
Best checkpoint number: 32, Last checkpoint number: 32
Copying the frozen checkpoint from ./frozen_models/agent/model_32.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'30'}
Training> Name=main_level/agent, Worker=0, Episode=681, Total reward=112.03, Steps=9665, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=682, Total reward=48.01, Steps=9675, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=683, Total reward=112.03, Steps=9698, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=684, Total reward=141.03, Steps=9727, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=685, Total reward=72.02, Steps=9742, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=686, Total reward=67.02, Steps=9756, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=687, Total reward=255.05, Steps=9807, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=688, Total reward=183.04, Steps=9844, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=689, Total reward=90.02, Steps=9863, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=690, Total reward=65.01, Steps=9876, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=691, Total reward=41.01, Steps=9885, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=692, Total reward=57.02, Steps=9897, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=693, Total reward=195.05, Steps=9937, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=694, Total reward=117.03, Steps=9961, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=695, Total reward=86.02, Steps=9979, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=696, Total reward=58.01, Steps=9991, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=697, Total reward=56.02, Steps=10003, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=698, Total reward=42.01, Steps=10012, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=699, Total reward=102.02, Steps=10033, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=700, Total reward=52.01, Steps=10044, Training iteration=34
Policy training> Surrogate loss=0.005401894450187683, KL divergence=0.00034436117857694626, Entropy=1.529739260673523, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.0012920560548081994, KL divergence=0.001997559331357479, Entropy=1.5132746696472168, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.025415444746613503, KL divergence=0.0070607843808829784, Entropy=1.5033327341079712, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02856997214257717, KL divergence=0.008085530251264572, Entropy=1.4879297018051147, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.014566920697689056, KL divergence=0.00680890865623951, Entropy=1.483108639717102, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0009369279141537845, KL divergence=0.007961644791066647, Entropy=1.4663594961166382, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.026254767552018166, KL divergence=0.008080387488007545, Entropy=1.4535584449768066, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.005817445460706949, KL divergence=0.007453506346791983, Entropy=1.465976357460022, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03425946086645126, KL divergence=0.005332247819751501, Entropy=1.4965440034866333, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02510693110525608, KL divergence=0.005075311753898859, Entropy=1.493484377861023, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/35_Step-10044.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/35_Step-10044.ckpt']
Uploaded 3 files for checkpoint 35 in 0.55 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_35.pb
Best checkpoint number: 33, Last checkpoint number: 33
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'31'}
Training> Name=main_level/agent, Worker=0, Episode=701, Total reward=52.01, Steps=10055, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=702, Total reward=40.01, Steps=10064, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=703, Total reward=72.02, Steps=10079, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=704, Total reward=163.04, Steps=10112, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=705, Total reward=133.03, Steps=10139, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=706, Total reward=58.01, Steps=10151, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=707, Total reward=66.02, Steps=10165, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=708, Total reward=332.07, Steps=10232, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=709, Total reward=86.02, Steps=10250, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=710, Total reward=137.03, Steps=10278, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=711, Total reward=223.05, Steps=10323, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=712, Total reward=77.02, Steps=10339, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=713, Total reward=117.03, Steps=10363, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=714, Total reward=129.03, Steps=10390, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=715, Total reward=66.02, Steps=10404, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=716, Total reward=83.02, Steps=10421, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=717, Total reward=37.01, Steps=10429, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=718, Total reward=126.03, Steps=10455, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=719, Total reward=68.02, Steps=10469, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=720, Total reward=30.01, Steps=10475, Training iteration=35
Policy training> Surrogate loss=-0.011815999634563923, KL divergence=0.002008051611483097, Entropy=1.4644831418991089, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.023464644327759743, KL divergence=0.019229410216212273, Entropy=1.3839222192764282, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.03782319650053978, KL divergence=0.01006102841347456, Entropy=1.4339901208877563, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023391103371977806, KL divergence=0.009746775962412357, Entropy=1.4181309938430786, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03648747131228447, KL divergence=0.011181291192770004, Entropy=1.391667366027832, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.019763747230172157, KL divergence=0.011945541016757488, Entropy=1.420811653137207, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.06298043578863144, KL divergence=0.010916157625615597, Entropy=1.4156370162963867, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.046045273542404175, KL divergence=0.012024630792438984, Entropy=1.3872219324111938, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.05686002969741821, KL divergence=0.01107634324580431, Entropy=1.3947159051895142, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03939786180853844, KL divergence=0.011476260609924793, Entropy=1.4106574058532715, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/36_Step-10475.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/36_Step-10475.ckpt']
Uploaded 3 files for checkpoint 36 in 0.50 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_36.pb
Best checkpoint number: 33, Last checkpoint number: 34
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'32'}
Training> Name=main_level/agent, Worker=0, Episode=721, Total reward=143.04, Steps=10505, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=722, Total reward=58.01, Steps=10517, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=723, Total reward=151.04, Steps=10548, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=724, Total reward=127.03, Steps=10574, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=725, Total reward=107.03, Steps=10596, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=726, Total reward=53.01, Steps=10607, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=727, Total reward=116.03, Steps=10631, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=728, Total reward=73.02, Steps=10646, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=729, Total reward=72.02, Steps=10661, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=730, Total reward=75.02, Steps=10676, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=731, Total reward=62.02, Steps=10689, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=732, Total reward=76.02, Steps=10705, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=733, Total reward=173.04, Steps=10740, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=734, Total reward=112.03, Steps=10763, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=735, Total reward=102.02, Steps=10784, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=736, Total reward=158.03, Steps=10816, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=737, Total reward=38.01, Steps=10824, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=738, Total reward=107.03, Steps=10846, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=739, Total reward=37.01, Steps=10854, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=740, Total reward=40.01, Steps=10862, Training iteration=36
Policy training> Surrogate loss=0.004299987107515335, KL divergence=0.0012547920923680067, Entropy=1.4599051475524902, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.008271720260381699, KL divergence=0.01413281261920929, Entropy=1.4871119260787964, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.01924058236181736, KL divergence=0.014114756137132645, Entropy=1.433840274810791, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02730613946914673, KL divergence=0.005702413618564606, Entropy=1.4142605066299438, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026928149163722992, KL divergence=0.01107784640043974, Entropy=1.4650102853775024, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.029702553525567055, KL divergence=0.01020655408501625, Entropy=1.448438048362732, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03628798946738243, KL divergence=0.007868103682994843, Entropy=1.4367600679397583, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03271350637078285, KL divergence=0.0138109615072608, Entropy=1.4373321533203125, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.046842318028211594, KL divergence=0.011512435972690582, Entropy=1.429416298866272, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.04012531042098999, KL divergence=0.007480120752006769, Entropy=1.4261912107467651, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/37_Step-10862.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/37_Step-10862.ckpt']
Uploaded 3 files for checkpoint 37 in 0.55 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_37.pb
Best checkpoint number: 33, Last checkpoint number: 35
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'34'}
Training> Name=main_level/agent, Worker=0, Episode=741, Total reward=182.05, Steps=10900, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=742, Total reward=92.02, Steps=10919, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=743, Total reward=87.02, Steps=10937, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=744, Total reward=157.04, Steps=10969, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=745, Total reward=124.03, Steps=10995, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=746, Total reward=43.01, Steps=11004, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=747, Total reward=186.04, Steps=11042, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=748, Total reward=47.01, Steps=11052, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=749, Total reward=135.03, Steps=11079, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=750, Total reward=41.01, Steps=11088, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=751, Total reward=198.04, Steps=11128, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=752, Total reward=120.04, Steps=11154, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=753, Total reward=66.02, Steps=11168, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=754, Total reward=62.02, Steps=11181, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=755, Total reward=109.03, Steps=11204, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=756, Total reward=73.02, Steps=11219, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=757, Total reward=200.04, Steps=11259, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=758, Total reward=112.03, Steps=11282, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=759, Total reward=90.02, Steps=11300, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=760, Total reward=123.03, Steps=11325, Training iteration=37
Policy training> Surrogate loss=0.009759639389812946, KL divergence=0.0010596056235954165, Entropy=1.4024327993392944, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.0008079037070274353, KL divergence=0.013698876835405827, Entropy=1.3202049732208252, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02441444993019104, KL divergence=0.009764638729393482, Entropy=1.3516379594802856, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.007182414177805185, KL divergence=0.008243771269917488, Entropy=1.3449089527130127, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.017993463203310966, KL divergence=0.010270093567669392, Entropy=1.3342669010162354, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.016420649364590645, KL divergence=0.008912323042750359, Entropy=1.3469812870025635, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028598502278327942, KL divergence=0.006498777773231268, Entropy=1.3503684997558594, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03586727753281593, KL divergence=0.009118069894611835, Entropy=1.3388797044754028, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.027012748643755913, KL divergence=0.008765780366957188, Entropy=1.3392369747161865, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03462500125169754, KL divergence=0.013522534631192684, Entropy=1.308133840560913, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/38_Step-11325.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/38_Step-11325.ckpt']
Uploaded 3 files for checkpoint 38 in 0.53 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_38.pb
Best checkpoint number: 33, Last checkpoint number: 36
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'35'}
Training> Name=main_level/agent, Worker=0, Episode=761, Total reward=241.05, Steps=11374, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=762, Total reward=111.03, Steps=11397, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=763, Total reward=196.05, Steps=11438, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=764, Total reward=57.02, Steps=11450, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=765, Total reward=118.03, Steps=11474, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=766, Total reward=43.01, Steps=11483, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=767, Total reward=290.06, Steps=11542, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=768, Total reward=47.01, Steps=11552, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=769, Total reward=398.09, Steps=11633, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=770, Total reward=98.02, Steps=11653, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=771, Total reward=137.03, Steps=11681, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=772, Total reward=92.02, Steps=11700, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=773, Total reward=52.01, Steps=11711, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=774, Total reward=195.05, Steps=11751, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=775, Total reward=177.04, Steps=11787, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=776, Total reward=253.06, Steps=11839, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=777, Total reward=47.01, Steps=11849, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=778, Total reward=80.02, Steps=11866, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=779, Total reward=205.05, Steps=11908, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=780, Total reward=47.01, Steps=11918, Training iteration=38
Policy training> Surrogate loss=0.0033281391952186823, KL divergence=0.006862032692879438, Entropy=1.2636462450027466, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01844874396920204, KL divergence=0.004204796627163887, Entropy=1.212944746017456, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.010242771357297897, KL divergence=0.005669796839356422, Entropy=1.2056372165679932, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.01900576800107956, KL divergence=0.009540539234876633, Entropy=1.2077356576919556, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.017159752547740936, KL divergence=0.006109586916863918, Entropy=1.2632958889007568, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03346921503543854, KL divergence=0.006292751990258694, Entropy=1.2240403890609741, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.008916586637496948, KL divergence=0.007997741922736168, Entropy=1.2341346740722656, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02248390018939972, KL divergence=0.005555989220738411, Entropy=1.2403596639633179, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03400161862373352, KL divergence=0.007023025769740343, Entropy=1.239530324935913, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.033382490277290344, KL divergence=0.008335995487868786, Entropy=1.2335766553878784, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/39_Step-11918.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/39_Step-11918.ckpt']
Uploaded 3 files for checkpoint 39 in 0.56 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_39.pb
Best checkpoint number: 33, Last checkpoint number: 37
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'36'}
Training> Name=main_level/agent, Worker=0, Episode=781, Total reward=47.01, Steps=11928, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=782, Total reward=57.02, Steps=11940, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=783, Total reward=97.02, Steps=11960, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=784, Total reward=143.04, Steps=11990, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=785, Total reward=100.03, Steps=12011, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=786, Total reward=43.01, Steps=12020, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=787, Total reward=113.03, Steps=12043, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=788, Total reward=234.05, Steps=12091, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=789, Total reward=43.01, Steps=12100, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=790, Total reward=75.02, Steps=12115, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=791, Total reward=45.01, Steps=12124, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=792, Total reward=97.02, Steps=12144, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=793, Total reward=263.06, Steps=12197, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=794, Total reward=270.06, Steps=12252, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=795, Total reward=57.02, Steps=12264, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=796, Total reward=84.02, Steps=12281, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=797, Total reward=120.04, Steps=12307, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=798, Total reward=148.04, Steps=12338, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=799, Total reward=393.09, Steps=12418, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=800, Total reward=111.03, Steps=12441, Training iteration=39
Policy training> Surrogate loss=0.004785214085131884, KL divergence=0.00018557212024461478, Entropy=1.2622482776641846, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.001669738907366991, KL divergence=0.011081084609031677, Entropy=1.282898187637329, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.011943835765123367, KL divergence=0.004762821830809116, Entropy=1.2326219081878662, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.008561979979276657, KL divergence=0.004641185514628887, Entropy=1.208626627922058, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.010544413700699806, KL divergence=0.00801907479763031, Entropy=1.2436467409133911, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.027399107813835144, KL divergence=0.003951002843677998, Entropy=1.2300865650177002, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.030457401648163795, KL divergence=0.005984200630337, Entropy=1.238204836845398, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02941998280584812, KL divergence=0.006087834481149912, Entropy=1.222382664680481, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.028517376631498337, KL divergence=0.0038438665214926004, Entropy=1.2135685682296753, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02303249202668667, KL divergence=0.009291719645261765, Entropy=1.226021409034729, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/40_Step-12441.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/40_Step-12441.ckpt']
Uploaded 3 files for checkpoint 40 in 0.54 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_40.pb
Best checkpoint number: 33, Last checkpoint number: 38
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'37'}
Training> Name=main_level/agent, Worker=0, Episode=801, Total reward=96.02, Steps=12461, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=802, Total reward=232.05, Steps=12508, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=803, Total reward=182.04, Steps=12545, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=804, Total reward=138.03, Steps=12573, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=805, Total reward=98.03, Steps=12594, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=806, Total reward=42.01, Steps=12603, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=807, Total reward=70.02, Steps=12618, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=808, Total reward=210.04, Steps=12660, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=809, Total reward=122.03, Steps=12685, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=810, Total reward=85.02, Steps=12702, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=811, Total reward=86.02, Steps=12720, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=812, Total reward=126.03, Steps=12746, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=813, Total reward=138.03, Steps=12774, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=814, Total reward=171.04, Steps=12809, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=815, Total reward=77.02, Steps=12825, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=816, Total reward=175.04, Steps=12861, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=817, Total reward=212.05, Steps=12904, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=818, Total reward=47.01, Steps=12914, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=819, Total reward=97.02, Steps=12934, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=820, Total reward=70.01, Steps=12948, Training iteration=40
Policy training> Surrogate loss=0.011054701171815395, KL divergence=0.006575084291398525, Entropy=1.2455207109451294, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.01074809767305851, KL divergence=0.021271059289574623, Entropy=1.2603813409805298, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.004671291448175907, KL divergence=0.0073112985119223595, Entropy=1.243792176246643, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.016262724995613098, KL divergence=0.00615882221609354, Entropy=1.2297505140304565, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.009319226257503033, KL divergence=0.011208044365048409, Entropy=1.2815204858779907, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.012336990796029568, KL divergence=0.0068938396871089935, Entropy=1.2863620519638062, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0252416729927063, KL divergence=0.004597227554768324, Entropy=1.250726342201233, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.012141942977905273, KL divergence=0.008022604510188103, Entropy=1.2127500772476196, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.01837194338440895, KL divergence=0.009457568638026714, Entropy=1.2393747568130493, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03225759044289589, KL divergence=0.007431027479469776, Entropy=1.2623929977416992, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/41_Step-12948.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/41_Step-12948.ckpt']
Uploaded 3 files for checkpoint 41 in 0.52 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_41.pb
Best checkpoint number: 33, Last checkpoint number: 39
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'38'}
Training> Name=main_level/agent, Worker=0, Episode=821, Total reward=111.03, Steps=12971, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=822, Total reward=247.05, Steps=13021, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=823, Total reward=87.02, Steps=13039, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=824, Total reward=178.04, Steps=13076, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=825, Total reward=86.02, Steps=13094, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=826, Total reward=38.01, Steps=13102, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=827, Total reward=87.02, Steps=13120, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=828, Total reward=190.04, Steps=13158, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=829, Total reward=61.02, Steps=13171, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=830, Total reward=42.01, Steps=13180, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=831, Total reward=107.03, Steps=13202, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=832, Total reward=166.04, Steps=13236, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=833, Total reward=188.04, Steps=13274, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=834, Total reward=235.06, Steps=13323, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=835, Total reward=109.03, Steps=13346, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=836, Total reward=42.01, Steps=13355, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=837, Total reward=117.03, Steps=13379, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=838, Total reward=165.03, Steps=13412, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=839, Total reward=85.02, Steps=13429, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=840, Total reward=50.02, Steps=13440, Training iteration=41
Policy training> Surrogate loss=0.0036222764756530523, KL divergence=0.003856221679598093, Entropy=1.184553861618042, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.005037096794694662, KL divergence=0.009978893212974072, Entropy=1.1224658489227295, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.008667255751788616, KL divergence=0.004255156964063644, Entropy=1.179909110069275, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.01765771582722664, KL divergence=0.003855446819216013, Entropy=1.1818903684616089, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029010113328695297, KL divergence=0.011613781563937664, Entropy=1.120383858680725, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.045163221657276154, KL divergence=0.007555360905826092, Entropy=1.1635017395019531, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.025255365297198296, KL divergence=0.00782798882573843, Entropy=1.1704176664352417, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.028304453939199448, KL divergence=0.005623639095574617, Entropy=1.1833690404891968, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.030774464830756187, KL divergence=0.00741623854264617, Entropy=1.1553508043289185, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.04327970743179321, KL divergence=0.00928807444870472, Entropy=1.1511220932006836, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/42_Step-13440.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/42_Step-13440.ckpt']
Uploaded 3 files for checkpoint 42 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_42.pb
Best checkpoint number: 33, Last checkpoint number: 40
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'39'}
Training> Name=main_level/agent, Worker=0, Episode=841, Total reward=51.02, Steps=13451, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=842, Total reward=61.02, Steps=13464, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=843, Total reward=143.03, Steps=13493, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=844, Total reward=56.02, Steps=13505, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=845, Total reward=72.02, Steps=13520, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=846, Total reward=58.01, Steps=13532, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=847, Total reward=198.04, Steps=13572, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=848, Total reward=176.04, Steps=13608, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=849, Total reward=92.02, Steps=13627, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=850, Total reward=110.02, Steps=13649, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=851, Total reward=30.01, Steps=13655, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=852, Total reward=202.04, Steps=13696, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=853, Total reward=253.06, Steps=13748, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=854, Total reward=77.02, Steps=13764, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=855, Total reward=111.03, Steps=13787, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=856, Total reward=96.02, Steps=13807, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=857, Total reward=61.02, Steps=13820, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=858, Total reward=157.04, Steps=13853, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=859, Total reward=137.03, Steps=13881, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=860, Total reward=55.01, Steps=13892, Training iteration=42
Policy training> Surrogate loss=0.000906919245608151, KL divergence=0.0038844961673021317, Entropy=1.1247341632843018, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.009604156017303467, KL divergence=0.01322851050645113, Entropy=1.095605492591858, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.0066294497810304165, KL divergence=0.007528894580900669, Entropy=1.1013085842132568, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.012169812805950642, KL divergence=0.008724045939743519, Entropy=1.104979157447815, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.008415834978222847, KL divergence=0.014721808955073357, Entropy=1.1142511367797852, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.017048794776201248, KL divergence=0.006645577494055033, Entropy=1.1105148792266846, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.020725419744849205, KL divergence=0.0063879042863845825, Entropy=1.1208257675170898, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.034339554607868195, KL divergence=0.010795609094202518, Entropy=1.1170731782913208, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.027153009548783302, KL divergence=0.007862838916480541, Entropy=1.0994846820831299, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03587091341614723, KL divergence=0.006698379758745432, Entropy=1.125908374786377, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/43_Step-13892.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/43_Step-13892.ckpt']
Uploaded 3 files for checkpoint 43 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_43.pb
Best checkpoint number: 33, Last checkpoint number: 41
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'40'}
Training> Name=main_level/agent, Worker=0, Episode=861, Total reward=95.02, Steps=13912, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=862, Total reward=48.01, Steps=13922, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=863, Total reward=297.06, Steps=13982, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=864, Total reward=172.04, Steps=14017, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=865, Total reward=128.03, Steps=14044, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=866, Total reward=291.07, Steps=14104, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=867, Total reward=195.05, Steps=14144, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=868, Total reward=143.03, Steps=14173, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=869, Total reward=118.03, Steps=14197, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=870, Total reward=85.02, Steps=14214, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=871, Total reward=117.03, Steps=14238, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=872, Total reward=86.02, Steps=14256, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=873, Total reward=282.07, Steps=14314, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=874, Total reward=47.01, Steps=14324, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=875, Total reward=81.02, Steps=14341, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=876, Total reward=102.02, Steps=14362, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=877, Total reward=167.04, Steps=14396, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=878, Total reward=127.03, Steps=14422, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=879, Total reward=98.02, Steps=14442, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=880, Total reward=45.01, Steps=14451, Training iteration=43
Policy training> Surrogate loss=-0.007394243963062763, KL divergence=0.004600707907229662, Entropy=1.123486876487732, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.002687394618988037, KL divergence=0.004681845661252737, Entropy=1.1236653327941895, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.006309261545538902, KL divergence=0.004442048724740744, Entropy=1.1368772983551025, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.0068778228014707565, KL divergence=0.008554231375455856, Entropy=1.1169624328613281, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0036154696717858315, KL divergence=0.008141886442899704, Entropy=1.0643329620361328, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.011465869843959808, KL divergence=0.005881530232727528, Entropy=1.0700329542160034, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.019156360998749733, KL divergence=0.006146718747913837, Entropy=1.1101324558258057, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=0.0016203001141548157, KL divergence=0.004744263365864754, Entropy=1.1007585525512695, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=0.016104133799672127, KL divergence=0.004815111868083477, Entropy=1.1100785732269287, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03089497797191143, KL divergence=0.00659777969121933, Entropy=1.080349326133728, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/44_Step-14451.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/44_Step-14451.ckpt']
Uploaded 3 files for checkpoint 44 in 0.52 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_44.pb
Best checkpoint number: 33, Last checkpoint number: 42
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'41'}
Training> Name=main_level/agent, Worker=0, Episode=881, Total reward=60.01, Steps=14463, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=882, Total reward=137.03, Steps=14491, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=883, Total reward=82.02, Steps=14508, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=884, Total reward=127.03, Steps=14534, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=885, Total reward=98.02, Steps=14554, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=886, Total reward=52.01, Steps=14565, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=887, Total reward=106.03, Steps=14587, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=888, Total reward=587.12, Steps=14705, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=889, Total reward=111.03, Steps=14728, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=890, Total reward=200.05, Steps=14769, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=891, Total reward=126.03, Steps=14795, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=892, Total reward=212.05, Steps=14838, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=893, Total reward=272.06, Steps=14893, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=894, Total reward=157.04, Steps=14925, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=895, Total reward=237.05, Steps=14973, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=896, Total reward=116.03, Steps=14997, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=897, Total reward=132.03, Steps=15024, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=898, Total reward=145.03, Steps=15053, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=899, Total reward=123.03, Steps=15078, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=900, Total reward=35.01, Steps=15085, Training iteration=44
Policy training> Surrogate loss=0.005593699868768454, KL divergence=0.0015049523208290339, Entropy=1.0838427543640137, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.028029730543494225, KL divergence=0.009229817427694798, Entropy=1.123577356338501, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.004064247012138367, KL divergence=0.00432952493429184, Entropy=1.0461983680725098, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.00453578494489193, KL divergence=0.0045828851871192455, Entropy=1.0519886016845703, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.005637118127197027, KL divergence=0.0043189069256186485, Entropy=1.0524137020111084, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.010890415869653225, KL divergence=0.005455857142806053, Entropy=1.0860650539398193, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.018522359430789948, KL divergence=0.004641719162464142, Entropy=1.0398542881011963, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.023331118747591972, KL divergence=0.004785397555679083, Entropy=1.0309910774230957, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.036216720938682556, KL divergence=0.0073076155968010426, Entropy=1.056250810623169, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0015636483440175653, KL divergence=0.0053413258865475655, Entropy=1.048903465270996, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/45_Step-15085.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/45_Step-15085.ckpt']
Uploaded 3 files for checkpoint 45 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_45.pb
Best checkpoint number: 33, Last checkpoint number: 43
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'42'}
Training> Name=main_level/agent, Worker=0, Episode=901, Total reward=110.02, Steps=15107, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=902, Total reward=146.03, Steps=15137, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=903, Total reward=263.06, Steps=15190, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=904, Total reward=82.02, Steps=15207, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=905, Total reward=120.03, Steps=15232, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=906, Total reward=42.01, Steps=15241, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=907, Total reward=213.05, Steps=15285, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=908, Total reward=225.05, Steps=15330, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=909, Total reward=96.02, Steps=15350, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=910, Total reward=90.02, Steps=15368, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=911, Total reward=98.02, Steps=15388, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=912, Total reward=116.03, Steps=15412, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=913, Total reward=206.05, Steps=15454, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=914, Total reward=197.04, Steps=15494, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=915, Total reward=82.02, Steps=15511, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=916, Total reward=152.03, Steps=15542, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=917, Total reward=100.03, Steps=15563, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=918, Total reward=201.05, Steps=15604, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=919, Total reward=126.03, Steps=15630, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=920, Total reward=123.03, Steps=15655, Training iteration=45
Policy training> Surrogate loss=0.012451143935322762, KL divergence=0.008742437697947025, Entropy=1.057326316833496, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.0027359947562217712, KL divergence=0.005402046255767345, Entropy=1.0483055114746094, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.034941501915454865, KL divergence=0.008195283822715282, Entropy=1.0787549018859863, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-7.033813744783401e-05, KL divergence=0.0036332113668322563, Entropy=1.070387840270996, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.020236577838659286, KL divergence=0.002492999192327261, Entropy=1.042090654373169, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.002369117457419634, KL divergence=0.005067323334515095, Entropy=1.0440807342529297, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.007281451486051083, KL divergence=0.004216582514345646, Entropy=1.0721994638442993, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02314816415309906, KL divergence=0.004136866424232721, Entropy=1.0715954303741455, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.001792706549167633, KL divergence=0.005478246603161097, Entropy=1.0563361644744873, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.00550301605835557, KL divergence=0.005756600759923458, Entropy=1.0465630292892456, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/46_Step-15655.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/46_Step-15655.ckpt']
Uploaded 3 files for checkpoint 46 in 0.44 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_46.pb
Best checkpoint number: 33, Last checkpoint number: 44
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'43'}
Training> Name=main_level/agent, Worker=0, Episode=921, Total reward=80.02, Steps=15671, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=922, Total reward=135.03, Steps=15699, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=923, Total reward=198.04, Steps=15739, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=924, Total reward=152.03, Steps=15770, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=925, Total reward=72.02, Steps=15785, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=926, Total reward=47.01, Steps=15795, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=927, Total reward=137.03, Steps=15823, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=928, Total reward=215.04, Steps=15866, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=929, Total reward=303.07, Steps=15928, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=930, Total reward=297.06, Steps=15988, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=931, Total reward=346.07, Steps=16058, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=932, Total reward=137.03, Steps=16086, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=933, Total reward=214.05, Steps=16130, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=934, Total reward=165.04, Steps=16164, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=935, Total reward=102.02, Steps=16185, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=936, Total reward=58.01, Steps=16197, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=937, Total reward=87.02, Steps=16215, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=938, Total reward=165.03, Steps=16248, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=939, Total reward=37.01, Steps=16256, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=940, Total reward=30.01, Steps=16262, Training iteration=46
Policy training> Surrogate loss=0.008167960681021214, KL divergence=0.006063709035515785, Entropy=1.1264339685440063, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014881134033203125, KL divergence=0.004915932193398476, Entropy=1.0619585514068604, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.0044019571505486965, KL divergence=0.008949019014835358, Entropy=1.03910493850708, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.011601854115724564, KL divergence=0.005186913534998894, Entropy=1.0877712965011597, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0011022670660167933, KL divergence=0.007370731793344021, Entropy=1.0520445108413696, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.014558709226548672, KL divergence=0.007412259001284838, Entropy=1.005638599395752, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03093988448381424, KL divergence=0.005852008704096079, Entropy=1.06544828414917, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.019699526950716972, KL divergence=0.010610022582113743, Entropy=1.0077171325683594, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.004281790927052498, KL divergence=0.007830548100173473, Entropy=1.0332837104797363, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.027773432433605194, KL divergence=0.007253756280988455, Entropy=1.0308985710144043, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/47_Step-16262.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/47_Step-16262.ckpt']
Uploaded 3 files for checkpoint 47 in 0.57 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_47.pb
Best checkpoint number: 33, Last checkpoint number: 45
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'44'}
Training> Name=main_level/agent, Worker=0, Episode=941, Total reward=90.02, Steps=16280, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=942, Total reward=67.02, Steps=16294, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=943, Total reward=71.02, Steps=16309, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=944, Total reward=222.05, Steps=16354, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=945, Total reward=47.01, Steps=16364, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=946, Total reward=43.01, Steps=16373, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=947, Total reward=188.04, Steps=16411, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=948, Total reward=225.05, Steps=16456, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=949, Total reward=127.03, Steps=16482, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=950, Total reward=102.02, Steps=16503, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=951, Total reward=527.11, Steps=16609, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=952, Total reward=38.01, Steps=16617, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=953, Total reward=210.06, Steps=16662, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=954, Total reward=198.04, Steps=16702, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=955, Total reward=218.05, Steps=16746, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=956, Total reward=167.04, Steps=16780, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=957, Total reward=88.02, Steps=16798, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=958, Total reward=47.01, Steps=16808, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=959, Total reward=151.04, Steps=16839, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=960, Total reward=62.02, Steps=16852, Training iteration=47
Policy training> Surrogate loss=-0.0008169065113179386, KL divergence=0.0008104258449748158, Entropy=1.0565979480743408, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.008882954716682434, KL divergence=0.008670173585414886, Entropy=1.0160685777664185, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.008771378546953201, KL divergence=0.00856697652488947, Entropy=0.974359393119812, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.017957326024770737, KL divergence=0.007032347377389669, Entropy=0.9923735857009888, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.010629462078213692, KL divergence=0.007923688739538193, Entropy=1.0002115964889526, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02396494895219803, KL divergence=0.00590491620823741, Entropy=0.9884770512580872, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0207257941365242, KL divergence=0.006617178674787283, Entropy=0.9897304177284241, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.010950956493616104, KL divergence=0.008965451270341873, Entropy=0.979823648929596, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.026754042133688927, KL divergence=0.0077054607681930065, Entropy=0.9971415400505066, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.031396061182022095, KL divergence=0.009887072257697582, Entropy=0.9563558101654053, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/48_Step-16852.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/48_Step-16852.ckpt']
Uploaded 3 files for checkpoint 48 in 0.49 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_48.pb
Best checkpoint number: 33, Last checkpoint number: 46
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'45'}
Training> Name=main_level/agent, Worker=0, Episode=961, Total reward=95.02, Steps=16871, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=962, Total reward=148.04, Steps=16902, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=963, Total reward=227.05, Steps=16948, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=964, Total reward=172.04, Steps=16983, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=965, Total reward=112.03, Steps=17006, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=966, Total reward=47.01, Steps=17016, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=967, Total reward=82.02, Steps=17033, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=968, Total reward=97.02, Steps=17053, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=969, Total reward=149.03, Steps=17083, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=970, Total reward=84.02, Steps=17101, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=971, Total reward=327.07, Steps=17168, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=972, Total reward=118.03, Steps=17192, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=973, Total reward=216.05, Steps=17236, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=974, Total reward=168.04, Steps=17271, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=975, Total reward=100.03, Steps=17292, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=976, Total reward=195.05, Steps=17332, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=977, Total reward=42.01, Steps=17341, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=978, Total reward=87.02, Steps=17359, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=979, Total reward=187.04, Steps=17397, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=980, Total reward=40.01, Steps=17405, Training iteration=48
Policy training> Surrogate loss=0.0061684176325798035, KL divergence=0.0017783426446840167, Entropy=0.9548156261444092, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.008346809074282646, KL divergence=0.018425554037094116, Entropy=0.8962581157684326, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.0331515334546566, KL divergence=0.006570722907781601, Entropy=0.9362725019454956, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.014983158558607101, KL divergence=0.008149202913045883, Entropy=0.9093990325927734, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0035841427743434906, KL divergence=0.005055591929703951, Entropy=0.9184223413467407, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.008073610253632069, KL divergence=0.011384671553969383, Entropy=0.9161719083786011, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.020340371876955032, KL divergence=0.005462964065372944, Entropy=0.9610289335250854, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.01553798746317625, KL divergence=0.008622834458947182, Entropy=0.9134781360626221, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.025625184178352356, KL divergence=0.008559668436646461, Entropy=0.9086058139801025, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.004506045952439308, KL divergence=0.005828370340168476, Entropy=0.9521057605743408, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/49_Step-17405.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/49_Step-17405.ckpt']
Uploaded 3 files for checkpoint 49 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_49.pb
Best checkpoint number: 33, Last checkpoint number: 47
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'46'}
Training> Name=main_level/agent, Worker=0, Episode=981, Total reward=60.01, Steps=17417, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=982, Total reward=246.05, Steps=17467, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=983, Total reward=95.02, Steps=17487, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=984, Total reward=142.03, Steps=17516, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=985, Total reward=197.04, Steps=17556, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=986, Total reward=47.01, Steps=17566, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=987, Total reward=229.06, Steps=17614, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=988, Total reward=208.04, Steps=17656, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=989, Total reward=219.05, Steps=17701, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=990, Total reward=80.02, Steps=17717, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=991, Total reward=312.07, Steps=17780, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=992, Total reward=141.03, Steps=17809, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=993, Total reward=67.02, Steps=17823, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=994, Total reward=229.05, Steps=17870, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=995, Total reward=227.05, Steps=17916, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=996, Total reward=290.07, Steps=17976, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=997, Total reward=92.02, Steps=17995, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=998, Total reward=325.08, Steps=18062, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=999, Total reward=115.02, Steps=18085, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=1000, Total reward=40.01, Steps=18093, Training iteration=49
Policy training> Surrogate loss=0.007665884681046009, KL divergence=0.001054175547324121, Entropy=0.9219511151313782, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.008516172878444195, KL divergence=0.007462158799171448, Entropy=0.9928151965141296, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.008557363413274288, KL divergence=0.0058100298047065735, Entropy=0.963105320930481, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.013267708010971546, KL divergence=0.002809481229633093, Entropy=0.9691289067268372, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.025470901280641556, KL divergence=0.006968209985643625, Entropy=0.9840772747993469, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.024949589744210243, KL divergence=0.0053421854972839355, Entropy=0.9400051832199097, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.01952514983713627, KL divergence=0.0040521202608942986, Entropy=0.9447426795959473, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.022248169407248497, KL divergence=0.0047827293165028095, Entropy=0.9910858273506165, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.022786179557442665, KL divergence=0.0038213604129850864, Entropy=0.9695631861686707, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.013974678702652454, KL divergence=0.0034838400315493345, Entropy=0.9408798217773438, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/50_Step-18093.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/50_Step-18093.ckpt']
Uploaded 3 files for checkpoint 50 in 0.54 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_50.pb
Best checkpoint number: 33, Last checkpoint number: 48
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'47'}
Training> Name=main_level/agent, Worker=0, Episode=1001, Total reward=80.02, Steps=18109, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1002, Total reward=77.02, Steps=18125, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1003, Total reward=198.04, Steps=18165, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1004, Total reward=107.03, Steps=18187, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1005, Total reward=102.02, Steps=18208, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1006, Total reward=33.01, Steps=18215, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1007, Total reward=107.03, Steps=18237, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1008, Total reward=190.04, Steps=18276, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1009, Total reward=133.04, Steps=18304, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1010, Total reward=87.02, Steps=18322, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1011, Total reward=367.08, Steps=18396, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1012, Total reward=258.06, Steps=18449, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1013, Total reward=162.04, Steps=18482, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1014, Total reward=173.04, Steps=18518, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1015, Total reward=42.01, Steps=18527, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1016, Total reward=83.02, Steps=18544, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1017, Total reward=112.03, Steps=18567, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1018, Total reward=161.04, Steps=18600, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1019, Total reward=260.06, Steps=18653, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1020, Total reward=45.01, Steps=18662, Training iteration=50
Policy training> Surrogate loss=0.0024122456088662148, KL divergence=0.00505281426012516, Entropy=0.9378650784492493, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.0010226741433143616, KL divergence=0.005712668877094984, Entropy=0.9022269248962402, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020943347364664078, KL divergence=0.0027340392116457224, Entropy=0.8694382905960083, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.007131125777959824, KL divergence=0.005110178142786026, Entropy=0.8786077499389648, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.014959867112338543, KL divergence=0.0036650861147791147, Entropy=0.9006824493408203, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.00604592077434063, KL divergence=0.003840371500700712, Entropy=0.9390563368797302, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.011108256876468658, KL divergence=0.0048964908346533775, Entropy=0.9405948519706726, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02380949817597866, KL divergence=0.007597597315907478, Entropy=0.9131009578704834, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.025175582617521286, KL divergence=0.006910854950547218, Entropy=0.9412150382995605, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.01916203834116459, KL divergence=0.005729047581553459, Entropy=0.9263367056846619, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/51_Step-18662.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/51_Step-18662.ckpt']
Uploaded 3 files for checkpoint 51 in 0.50 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_51.pb
Best checkpoint number: 33, Last checkpoint number: 49
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'48'}
Training> Name=main_level/agent, Worker=0, Episode=1021, Total reward=70.01, Steps=18676, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1022, Total reward=15.0, Steps=18679, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1023, Total reward=197.04, Steps=18719, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1024, Total reward=147.03, Steps=18749, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1025, Total reward=118.03, Steps=18773, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1026, Total reward=33.01, Steps=18780, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1027, Total reward=250.05, Steps=18830, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1028, Total reward=225.05, Steps=18875, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1029, Total reward=46.01, Steps=18885, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1030, Total reward=420.09, Steps=18970, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1031, Total reward=431.09, Steps=19057, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1032, Total reward=91.02, Steps=19076, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1033, Total reward=222.05, Steps=19121, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1034, Total reward=409.08, Steps=19203, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1035, Total reward=51.02, Steps=19214, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1036, Total reward=82.02, Steps=19231, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1037, Total reward=96.02, Steps=19251, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1038, Total reward=147.03, Steps=19281, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1039, Total reward=223.05, Steps=19326, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1040, Total reward=162.04, Steps=19359, Training iteration=51
Policy training> Surrogate loss=-0.014188183471560478, KL divergence=0.006973976735025644, Entropy=0.9419611096382141, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.00943236518651247, KL divergence=0.006598050240427256, Entropy=0.9564129114151001, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.008198320865631104, KL divergence=0.004378506448119879, Entropy=0.922592282295227, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.015527020208537579, KL divergence=0.010327264666557312, Entropy=0.9732527732849121, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.017098193988204002, KL divergence=0.0035745755303651094, Entropy=0.9414526224136353, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.019600901752710342, KL divergence=0.00965075008571148, Entropy=0.9832280278205872, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.032055605202913284, KL divergence=0.004172400571405888, Entropy=0.9455232620239258, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.013018297031521797, KL divergence=0.006587905343621969, Entropy=0.9547303318977356, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02504245936870575, KL divergence=0.006646422203630209, Entropy=0.9659504890441895, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.013406144455075264, KL divergence=0.007185677532106638, Entropy=0.9759372472763062, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/52_Step-19359.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/52_Step-19359.ckpt']
Uploaded 3 files for checkpoint 52 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_52.pb
Best checkpoint number: 33, Last checkpoint number: 50
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'49'}
Training> Name=main_level/agent, Worker=0, Episode=1041, Total reward=42.01, Steps=19368, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1042, Total reward=157.04, Steps=19400, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1043, Total reward=92.02, Steps=19419, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1044, Total reward=188.04, Steps=19457, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1045, Total reward=98.02, Steps=19477, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1046, Total reward=47.01, Steps=19487, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1047, Total reward=102.02, Steps=19508, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1048, Total reward=178.04, Steps=19544, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1049, Total reward=77.02, Steps=19560, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1050, Total reward=90.02, Steps=19578, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1051, Total reward=172.04, Steps=19613, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1052, Total reward=448.1, Steps=19705, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1053, Total reward=478.1, Steps=19801, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1054, Total reward=127.03, Steps=19827, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1055, Total reward=97.02, Steps=19847, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1056, Total reward=52.01, Steps=19858, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1057, Total reward=117.03, Steps=19882, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1058, Total reward=171.04, Steps=19917, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1059, Total reward=153.04, Steps=19949, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1060, Total reward=40.01, Steps=19957, Training iteration=52
Policy training> Surrogate loss=0.0014337566681206226, KL divergence=0.002968837507069111, Entropy=0.9517076015472412, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.005544150248169899, KL divergence=0.006578178144991398, Entropy=0.9235522747039795, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.006752461660653353, KL divergence=0.0040580290369689465, Entropy=0.9759342670440674, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.017322072759270668, KL divergence=0.007073183078318834, Entropy=0.9430184960365295, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.008187197148799896, KL divergence=0.0025172478053718805, Entropy=0.9414128065109253, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.010580967180430889, KL divergence=0.010285669937729836, Entropy=0.9809109568595886, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02674417570233345, KL divergence=0.0031056718435138464, Entropy=0.9432893395423889, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.016018953174352646, KL divergence=0.0024402481503784657, Entropy=0.9709776639938354, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.0034056338481605053, KL divergence=0.005103289615362883, Entropy=0.9682453870773315, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.011619867756962776, KL divergence=0.0034991709981113672, Entropy=0.9658759236335754, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/53_Step-19957.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/53_Step-19957.ckpt']
Uploaded 3 files for checkpoint 53 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_53.pb
Best checkpoint number: 33, Last checkpoint number: 51
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'50'}
Training> Name=main_level/agent, Worker=0, Episode=1061, Total reward=229.06, Steps=20005, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1062, Total reward=166.04, Steps=20039, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1063, Total reward=97.02, Steps=20059, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1064, Total reward=187.04, Steps=20097, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1065, Total reward=78.02, Steps=20113, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1066, Total reward=43.01, Steps=20122, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1067, Total reward=111.03, Steps=20145, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1068, Total reward=220.04, Steps=20189, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1069, Total reward=120.03, Steps=20214, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1070, Total reward=192.04, Steps=20253, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1071, Total reward=173.04, Steps=20288, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1072, Total reward=362.08, Steps=20361, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1073, Total reward=277.06, Steps=20417, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1074, Total reward=171.04, Steps=20452, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1075, Total reward=147.03, Steps=20482, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1076, Total reward=102.02, Steps=20503, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1077, Total reward=107.03, Steps=20525, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1078, Total reward=165.03, Steps=20558, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1079, Total reward=41.01, Steps=20567, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1080, Total reward=366.08, Steps=20641, Training iteration=53
Policy training> Surrogate loss=-0.0026517906226217747, KL divergence=0.0013360598823055625, Entropy=0.9290701150894165, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.004551571793854237, KL divergence=0.0063973539508879185, Entropy=0.8567602038383484, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.00914639700204134, KL divergence=0.004323653876781464, Entropy=0.8759824633598328, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.014537855982780457, KL divergence=0.0035902292001992464, Entropy=0.9068740606307983, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.004061090759932995, KL divergence=0.004865154158324003, Entropy=0.8802269101142883, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.016567233949899673, KL divergence=0.004546052776277065, Entropy=0.8996454477310181, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.01358350645750761, KL divergence=0.006695867516100407, Entropy=0.9012872576713562, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.007080258335918188, KL divergence=0.006645129527896643, Entropy=0.9067057371139526, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02745840884745121, KL divergence=0.004511556122452021, Entropy=0.9085016250610352, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.016277505084872246, KL divergence=0.005940414033830166, Entropy=0.8920281529426575, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/54_Step-20641.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/54_Step-20641.ckpt']
Uploaded 3 files for checkpoint 54 in 0.58 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_54.pb
Best checkpoint number: 33, Last checkpoint number: 52
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'51'}
Training> Name=main_level/agent, Worker=0, Episode=1081, Total reward=70.01, Steps=20655, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1082, Total reward=148.03, Steps=20685, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1083, Total reward=87.02, Steps=20703, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1084, Total reward=42.01, Steps=20712, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1085, Total reward=102.02, Steps=20733, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1086, Total reward=46.01, Steps=20743, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1087, Total reward=121.03, Steps=20768, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1088, Total reward=197.04, Steps=20808, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1089, Total reward=160.03, Steps=20840, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1090, Total reward=90.02, Steps=20858, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1091, Total reward=193.04, Steps=20897, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1092, Total reward=393.09, Steps=20977, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1093, Total reward=67.02, Steps=20991, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1094, Total reward=146.03, Steps=21021, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1095, Total reward=158.04, Steps=21054, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1096, Total reward=167.04, Steps=21088, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1097, Total reward=222.05, Steps=21133, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1098, Total reward=157.04, Steps=21165, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1099, Total reward=120.02, Steps=21189, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1100, Total reward=150.04, Steps=21220, Training iteration=54
Policy training> Surrogate loss=0.000516774773132056, KL divergence=0.002757605630904436, Entropy=0.9555394649505615, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.0009553920826874673, KL divergence=0.0037164324894547462, Entropy=0.9804671406745911, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.0015948733780533075, KL divergence=0.006082015112042427, Entropy=0.9069579243659973, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.010128644295036793, KL divergence=0.0031821741722524166, Entropy=0.8906885981559753, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.004994372371584177, KL divergence=0.003842997131869197, Entropy=0.8818062543869019, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.01323650497943163, KL divergence=0.0044578080996870995, Entropy=0.8949871063232422, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.01709877699613571, KL divergence=0.005056716967374086, Entropy=0.8880020380020142, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.01500198245048523, KL divergence=0.005720434710383415, Entropy=0.8762066960334778, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.014355121180415154, KL divergence=0.005019830539822578, Entropy=0.898549497127533, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.017552031204104424, KL divergence=0.007134521380066872, Entropy=0.8813488483428955, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/55_Step-21220.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/55_Step-21220.ckpt']
Uploaded 3 files for checkpoint 55 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_55.pb
Best checkpoint number: 33, Last checkpoint number: 53
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'52'}
Training> Name=main_level/agent, Worker=0, Episode=1101, Total reward=236.05, Steps=21268, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1102, Total reward=155.04, Steps=21300, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1103, Total reward=333.07, Steps=21367, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1104, Total reward=131.03, Steps=21394, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1105, Total reward=203.04, Steps=21435, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1106, Total reward=42.01, Steps=21444, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1107, Total reward=153.04, Steps=21476, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1108, Total reward=132.03, Steps=21503, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1109, Total reward=140.03, Steps=21531, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1110, Total reward=85.02, Steps=21548, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1111, Total reward=97.02, Steps=21568, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1112, Total reward=400.09, Steps=21650, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1113, Total reward=263.06, Steps=21703, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1114, Total reward=207.05, Steps=21745, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1115, Total reward=132.03, Steps=21772, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1116, Total reward=321.07, Steps=21837, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1117, Total reward=513.11, Steps=21941, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1118, Total reward=265.05, Steps=21994, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1119, Total reward=180.04, Steps=22030, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1120, Total reward=35.01, Steps=22037, Training iteration=55
Policy training> Surrogate loss=-0.011486899107694626, KL divergence=0.0031649682205170393, Entropy=0.8876448273658752, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.008176826871931553, KL divergence=0.004645169712603092, Entropy=0.8328601717948914, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.003129269229248166, KL divergence=0.00429235864430666, Entropy=0.8482591509819031, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.013753439299762249, KL divergence=0.00726109417155385, Entropy=0.8634678721427917, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.006312083452939987, KL divergence=0.004338061902672052, Entropy=0.8402584195137024, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026232531294226646, KL divergence=0.008547984063625336, Entropy=0.8617474436759949, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029438583180308342, KL divergence=0.004938209895044565, Entropy=0.8513760566711426, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=0.0037152741570025682, KL divergence=0.005284967366605997, Entropy=0.8424777388572693, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.018580665811896324, KL divergence=0.006415675859898329, Entropy=0.8422468304634094, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02281568944454193, KL divergence=0.004524603020399809, Entropy=0.8663274645805359, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/56_Step-22037.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/56_Step-22037.ckpt']
Uploaded 3 files for checkpoint 56 in 0.48 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_56.pb
Best checkpoint number: 33, Last checkpoint number: 54
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'53'}
Training> Name=main_level/agent, Worker=0, Episode=1121, Total reward=90.02, Steps=22055, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1122, Total reward=179.04, Steps=22092, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1123, Total reward=153.03, Steps=22123, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1124, Total reward=92.02, Steps=22142, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1125, Total reward=137.03, Steps=22170, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1126, Total reward=307.07, Steps=22233, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1127, Total reward=138.03, Steps=22261, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1128, Total reward=202.04, Steps=22302, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1129, Total reward=111.03, Steps=22325, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1130, Total reward=95.02, Steps=22344, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1131, Total reward=30.01, Steps=22350, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1132, Total reward=502.11, Steps=22452, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1133, Total reward=476.1, Steps=22548, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1134, Total reward=461.1, Steps=22641, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1135, Total reward=227.05, Steps=22687, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1136, Total reward=273.06, Steps=22742, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1137, Total reward=467.1, Steps=22837, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1138, Total reward=405.1, Steps=22921, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1139, Total reward=214.04, Steps=22964, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1140, Total reward=52.01, Steps=22975, Training iteration=56
Policy training> Surrogate loss=0.002730573993176222, KL divergence=0.004984195344150066, Entropy=0.8053847551345825, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.004915016237646341, KL divergence=0.006341096013784409, Entropy=0.7969053387641907, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.01554157119244337, KL divergence=0.006257144268602133, Entropy=0.7698361277580261, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.0021732875611633062, KL divergence=0.008398234844207764, Entropy=0.7859365344047546, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.007339630275964737, KL divergence=0.004552535247057676, Entropy=0.7993306517601013, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.002928274217993021, KL divergence=0.008594456128776073, Entropy=0.8032665848731995, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.004656717646867037, KL divergence=0.006665912922471762, Entropy=0.788230299949646, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.024847036227583885, KL divergence=0.005851235706359148, Entropy=0.7895752787590027, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.015242679044604301, KL divergence=0.005130944307893515, Entropy=0.7807410955429077, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.01905309222638607, KL divergence=0.003701999317854643, Entropy=0.7879007458686829, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/57_Step-22975.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/57_Step-22975.ckpt']
Uploaded 3 files for checkpoint 57 in 0.55 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_57.pb
Best checkpoint number: 33, Last checkpoint number: 55
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'54'}
Training> Name=main_level/agent, Worker=0, Episode=1141, Total reward=75.02, Steps=22990, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1142, Total reward=52.01, Steps=23001, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1143, Total reward=77.02, Steps=23017, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1144, Total reward=218.05, Steps=23062, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1145, Total reward=109.03, Steps=23085, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1146, Total reward=101.02, Steps=23106, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1147, Total reward=296.06, Steps=23166, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1148, Total reward=279.06, Steps=23223, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1149, Total reward=242.06, Steps=23273, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1150, Total reward=105.02, Steps=23294, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1151, Total reward=367.08, Steps=23368, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1152, Total reward=387.08, Steps=23446, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1153, Total reward=146.03, Steps=23476, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1154, Total reward=237.05, Steps=23524, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1155, Total reward=100.03, Steps=23545, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1156, Total reward=127.03, Steps=23571, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1157, Total reward=42.01, Steps=23580, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1158, Total reward=187.04, Steps=23618, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1159, Total reward=178.05, Steps=23656, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1160, Total reward=155.03, Steps=23687, Training iteration=57
Policy training> Surrogate loss=0.009641592390835285, KL divergence=0.0030540376901626587, Entropy=0.7454060912132263, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.0069567738100886345, KL divergence=0.004844181705266237, Entropy=0.7090799808502197, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.0062833442352712154, KL divergence=0.0039055421948432922, Entropy=0.7033984065055847, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.008964535780251026, KL divergence=0.0037851862143725157, Entropy=0.717704176902771, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0109088821336627, KL divergence=0.006931825075298548, Entropy=0.7045634388923645, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.00018365545838605613, KL divergence=0.0024375170469284058, Entropy=0.7524240016937256, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.015858953818678856, KL divergence=0.0039827097207307816, Entropy=0.7182024717330933, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.008208551444113255, KL divergence=0.004000704735517502, Entropy=0.7293067574501038, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.018881360068917274, KL divergence=0.004022474866360426, Entropy=0.7278829216957092, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.013303812593221664, KL divergence=0.005485203582793474, Entropy=0.7250499129295349, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/58_Step-23687.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/58_Step-23687.ckpt']
Uploaded 3 files for checkpoint 58 in 0.53 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_58.pb
Best checkpoint number: 33, Last checkpoint number: 56
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'55'}
Training> Name=main_level/agent, Worker=0, Episode=1161, Total reward=75.02, Steps=23702, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1162, Total reward=217.05, Steps=23746, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1163, Total reward=153.03, Steps=23777, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1164, Total reward=122.03, Steps=23802, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1165, Total reward=51.02, Steps=23813, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1166, Total reward=207.05, Steps=23856, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1167, Total reward=66.02, Steps=23870, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1168, Total reward=142.03, Steps=23899, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1169, Total reward=254.06, Steps=23951, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1170, Total reward=80.02, Steps=23967, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1171, Total reward=45.01, Steps=23976, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1172, Total reward=87.02, Steps=23994, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1173, Total reward=62.02, Steps=24007, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1174, Total reward=262.06, Steps=24060, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1175, Total reward=147.03, Steps=24090, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1176, Total reward=420.08, Steps=24174, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1177, Total reward=122.03, Steps=24199, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1178, Total reward=145.04, Steps=24229, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1179, Total reward=83.02, Steps=24246, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1180, Total reward=140.03, Steps=24274, Training iteration=58
Policy training> Surrogate loss=-0.003282591700553894, KL divergence=0.0011621471494436264, Entropy=0.7262693047523499, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.0012197908945381641, KL divergence=0.006644161883741617, Entropy=0.6877601146697998, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.006776995956897736, KL divergence=0.004932829644531012, Entropy=0.705253541469574, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.013122384436428547, KL divergence=0.006041406653821468, Entropy=0.6892074346542358, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.022593431174755096, KL divergence=0.004418137017637491, Entropy=0.7083137035369873, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.016840526834130287, KL divergence=0.005021465942263603, Entropy=0.7106048464775085, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.008818359114229679, KL divergence=0.008071929216384888, Entropy=0.6880330443382263, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0059862276539206505, KL divergence=0.004655895754694939, Entropy=0.7013131380081177, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.025190534070134163, KL divergence=0.009607803076505661, Entropy=0.6903760433197021, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.01476157084107399, KL divergence=0.007392607629299164, Entropy=0.692867636680603, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/59_Step-24274.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/59_Step-24274.ckpt']
Uploaded 3 files for checkpoint 59 in 0.56 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_59.pb
Best checkpoint number: 33, Last checkpoint number: 57
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'56'}
Training> Name=main_level/agent, Worker=0, Episode=1181, Total reward=75.02, Steps=24289, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1182, Total reward=142.03, Steps=24318, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1183, Total reward=47.01, Steps=24328, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1184, Total reward=179.04, Steps=24365, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1185, Total reward=110.03, Steps=24388, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1186, Total reward=38.01, Steps=24396, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1187, Total reward=148.03, Steps=24426, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1188, Total reward=201.05, Steps=24467, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1189, Total reward=150.03, Steps=24497, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1190, Total reward=92.02, Steps=24516, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1191, Total reward=137.03, Steps=24544, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1192, Total reward=684.15, Steps=24683, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1193, Total reward=56.02, Steps=24695, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1194, Total reward=157.04, Steps=24727, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1195, Total reward=388.09, Steps=24806, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1196, Total reward=102.02, Steps=24827, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1197, Total reward=47.01, Steps=24837, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1198, Total reward=155.03, Steps=24868, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1199, Total reward=37.01, Steps=24876, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1200, Total reward=52.01, Steps=24887, Training iteration=59
Policy training> Surrogate loss=-0.007176809944212437, KL divergence=0.001033684122376144, Entropy=0.7495290637016296, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01844125986099243, KL divergence=0.008737251162528992, Entropy=0.7944753766059875, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.016958795487880707, KL divergence=0.004590059630572796, Entropy=0.7469257712364197, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.007518106140196323, KL divergence=0.004017424304038286, Entropy=0.7366533279418945, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.003987654112279415, KL divergence=0.006327923387289047, Entropy=0.7432270646095276, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.001545874634757638, KL divergence=0.007837088778614998, Entropy=0.7328493595123291, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.012011457234621048, KL divergence=0.009042531251907349, Entropy=0.7236904501914978, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03941865637898445, KL divergence=0.011771480552852154, Entropy=0.7528043389320374, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.018713338300585747, KL divergence=0.012837965972721577, Entropy=0.76651930809021, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.017264442518353462, KL divergence=0.01674642413854599, Entropy=0.7749449610710144, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/60_Step-24887.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/60_Step-24887.ckpt']
Uploaded 3 files for checkpoint 60 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_60.pb
Best checkpoint number: 33, Last checkpoint number: 58
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'57'}
Training> Name=main_level/agent, Worker=0, Episode=1201, Total reward=229.05, Steps=24934, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1202, Total reward=212.05, Steps=24977, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1203, Total reward=196.04, Steps=25017, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1204, Total reward=184.04, Steps=25055, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1205, Total reward=61.02, Steps=25068, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1206, Total reward=117.03, Steps=25092, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1207, Total reward=280.06, Steps=25148, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1208, Total reward=225.05, Steps=25193, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1209, Total reward=585.13, Steps=25312, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1210, Total reward=90.02, Steps=25330, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1211, Total reward=36.01, Steps=25338, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1212, Total reward=292.06, Steps=25397, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1213, Total reward=232.05, Steps=25444, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1214, Total reward=495.1, Steps=25544, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1215, Total reward=106.03, Steps=25566, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1216, Total reward=793.18, Steps=25728, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1217, Total reward=265.05, Steps=25781, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1218, Total reward=156.04, Steps=25813, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1219, Total reward=106.03, Steps=25835, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=1220, Total reward=140.03, Steps=25863, Training iteration=60
Policy training> Surrogate loss=0.0005950492923147976, KL divergence=0.0031532554421573877, Entropy=0.6853111982345581, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.005321630742400885, KL divergence=0.007146110292524099, Entropy=0.7304506301879883, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.00425462331622839, KL divergence=0.005237349774688482, Entropy=0.6699371337890625, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.004982693586498499, KL divergence=0.006069399416446686, Entropy=0.7451450824737549, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.005307671148329973, KL divergence=0.00454086484387517, Entropy=0.6943700313568115, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.009996344335377216, KL divergence=0.006012199446558952, Entropy=0.7398130893707275, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.011601122096180916, KL divergence=0.005902084521949291, Entropy=0.7097901105880737, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.011555586010217667, KL divergence=0.004249964840710163, Entropy=0.7311547994613647, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.011395524255931377, KL divergence=0.0036812308244407177, Entropy=0.7261664271354675, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.025036556646227837, KL divergence=0.004027364309877157, Entropy=0.7218124270439148, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/61_Step-25863.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/61_Step-25863.ckpt']
Uploaded 3 files for checkpoint 61 in 0.54 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_61.pb
Best checkpoint number: 33, Last checkpoint number: 59
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'58'}
Training> Name=main_level/agent, Worker=0, Episode=1221, Total reward=52.01, Steps=25874, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1222, Total reward=229.05, Steps=25921, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1223, Total reward=112.03, Steps=25944, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1224, Total reward=153.03, Steps=25975, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1225, Total reward=108.02, Steps=25997, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1226, Total reward=129.03, Steps=26024, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1227, Total reward=121.03, Steps=26049, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1228, Total reward=92.02, Steps=26068, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1229, Total reward=321.07, Steps=26133, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1230, Total reward=267.06, Steps=26187, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1231, Total reward=364.09, Steps=26263, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1232, Total reward=337.07, Steps=26331, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1233, Total reward=131.03, Steps=26358, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1234, Total reward=190.04, Steps=26397, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1235, Total reward=222.05, Steps=26443, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1236, Total reward=46.01, Steps=26453, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1237, Total reward=113.03, Steps=26476, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1238, Total reward=122.03, Steps=26502, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1239, Total reward=37.01, Steps=26510, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=1240, Total reward=45.01, Steps=26519, Training iteration=61
Policy training> Surrogate loss=0.009477972984313965, KL divergence=0.002561217173933983, Entropy=0.8248418569564819, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.011900994926691055, KL divergence=0.001615965156815946, Entropy=0.8080825805664062, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.0041653988882899284, KL divergence=0.006642249878495932, Entropy=0.8094226717948914, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.01167636550962925, KL divergence=0.003997187130153179, Entropy=0.7949352264404297, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.005706889554858208, KL divergence=0.0045434655621647835, Entropy=0.7999533414840698, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.025061458349227905, KL divergence=0.003308552084490657, Entropy=0.7849680781364441, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.015467723831534386, KL divergence=0.004132316913455725, Entropy=0.7938332557678223, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02266581542789936, KL divergence=0.0059661646373569965, Entropy=0.7815176844596863, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.023224782198667526, KL divergence=0.004687715321779251, Entropy=0.8150900602340698, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.028102267533540726, KL divergence=0.005633019842207432, Entropy=0.8047963380813599, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/62_Step-26519.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/62_Step-26519.ckpt']
Uploaded 3 files for checkpoint 62 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_62.pb
Best checkpoint number: 33, Last checkpoint number: 60
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'59'}
Training> Name=main_level/agent, Worker=0, Episode=1241, Total reward=41.01, Steps=26528, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1242, Total reward=178.04, Steps=26564, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1243, Total reward=147.03, Steps=26594, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1244, Total reward=147.03, Steps=26624, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1245, Total reward=112.03, Steps=26647, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1246, Total reward=87.02, Steps=26665, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1247, Total reward=291.06, Steps=26724, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1248, Total reward=57.02, Steps=26736, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1249, Total reward=152.04, Steps=26768, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1250, Total reward=90.02, Steps=26786, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1251, Total reward=343.08, Steps=26856, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1252, Total reward=306.07, Steps=26918, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1253, Total reward=345.07, Steps=26988, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1254, Total reward=70.02, Steps=27003, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1255, Total reward=384.08, Steps=27080, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1256, Total reward=87.02, Steps=27098, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1257, Total reward=340.08, Steps=27168, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1258, Total reward=52.01, Steps=27179, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1259, Total reward=117.03, Steps=27203, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=1260, Total reward=81.03, Steps=27221, Training iteration=62
Policy training> Surrogate loss=0.03127215430140495, KL divergence=0.0022113136947155, Entropy=0.8010677099227905, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.01347436010837555, KL divergence=0.0056083910167217255, Entropy=0.7791174650192261, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.03310607373714447, KL divergence=0.005324139259755611, Entropy=0.7672011852264404, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.010174855589866638, KL divergence=0.005555185489356518, Entropy=0.7632498741149902, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.015264342539012432, KL divergence=0.004445821046829224, Entropy=0.7721636295318604, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.027391713112592697, KL divergence=0.005645264405757189, Entropy=0.7569178342819214, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.009243101812899113, KL divergence=0.004825366195291281, Entropy=0.7436001896858215, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.014051620848476887, KL divergence=0.00538175692781806, Entropy=0.7687867879867554, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.031431104987859726, KL divergence=0.006059975363314152, Entropy=0.7544837594032288, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.021194253116846085, KL divergence=0.004815974738448858, Entropy=0.779191792011261, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/63_Step-27221.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/63_Step-27221.ckpt']
Uploaded 3 files for checkpoint 63 in 0.52 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_63.pb
Best checkpoint number: 33, Last checkpoint number: 61
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'60'}
Training> Name=main_level/agent, Worker=0, Episode=1261, Total reward=57.02, Steps=27233, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1262, Total reward=211.05, Steps=27276, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1263, Total reward=172.04, Steps=27311, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1264, Total reward=83.02, Steps=27328, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1265, Total reward=99.03, Steps=27349, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1266, Total reward=42.01, Steps=27358, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1267, Total reward=300.06, Steps=27418, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1268, Total reward=206.05, Steps=27461, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1269, Total reward=160.03, Steps=27493, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1270, Total reward=65.02, Steps=27507, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1271, Total reward=408.09, Steps=27590, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1272, Total reward=352.07, Steps=27661, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1273, Total reward=363.07, Steps=27734, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1274, Total reward=208.04, Steps=27776, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1275, Total reward=460.12, Steps=27872, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1276, Total reward=87.02, Steps=27890, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1277, Total reward=260.06, Steps=27943, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1278, Total reward=150.04, Steps=27974, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1279, Total reward=248.05, Steps=28024, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=1280, Total reward=135.03, Steps=28051, Training iteration=63
Policy training> Surrogate loss=0.007929203100502491, KL divergence=0.00509371655061841, Entropy=0.6866963505744934, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.006078678648918867, KL divergence=0.007182254921644926, Entropy=0.6968040466308594, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.016704818233847618, KL divergence=0.0039535765536129475, Entropy=0.7407477498054504, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.007896874099969864, KL divergence=0.00485559506341815, Entropy=0.7110881805419922, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0010151118040084839, KL divergence=0.004220090806484222, Entropy=0.7363786101341248, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03378443047404289, KL divergence=0.006421955768018961, Entropy=0.7070518136024475, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=0.009879336692392826, KL divergence=0.005144229158759117, Entropy=0.7083771228790283, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.013418135233223438, KL divergence=0.003864691825583577, Entropy=0.7273933291435242, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.024787267670035362, KL divergence=0.004941309802234173, Entropy=0.7166192531585693, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.026011457666754723, KL divergence=0.00486943731084466, Entropy=0.7360305190086365, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/64_Step-28051.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/64_Step-28051.ckpt']
Uploaded 3 files for checkpoint 64 in 0.55 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_64.pb
Best checkpoint number: 33, Last checkpoint number: 62
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'61'}
Training> Name=main_level/agent, Worker=0, Episode=1281, Total reward=37.01, Steps=28059, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1282, Total reward=173.04, Steps=28094, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1283, Total reward=152.03, Steps=28125, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1284, Total reward=81.02, Steps=28142, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1285, Total reward=103.02, Steps=28163, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1286, Total reward=48.01, Steps=28173, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1287, Total reward=645.13, Steps=28303, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1288, Total reward=188.04, Steps=28341, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1289, Total reward=145.03, Steps=28370, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1290, Total reward=489.1, Steps=28469, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1291, Total reward=188.04, Steps=28507, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1292, Total reward=286.06, Steps=28565, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1293, Total reward=214.04, Steps=28608, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1294, Total reward=276.06, Steps=28664, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1295, Total reward=102.02, Steps=28685, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1296, Total reward=446.09, Steps=28775, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1297, Total reward=42.01, Steps=28784, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1298, Total reward=145.03, Steps=28813, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1299, Total reward=57.02, Steps=28825, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=1300, Total reward=145.03, Steps=28854, Training iteration=64
Policy training> Surrogate loss=0.002379918470978737, KL divergence=0.001136407139711082, Entropy=0.6741857528686523, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.004830304533243179, KL divergence=0.004527829121798277, Entropy=0.6423370242118835, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.0077721551060676575, KL divergence=0.0033646353986114264, Entropy=0.644629716873169, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.0040218825452029705, KL divergence=0.0031612636521458626, Entropy=0.6556547284126282, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.01535310223698616, KL divergence=0.002489931881427765, Entropy=0.6702651977539062, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.01833530329167843, KL divergence=0.005029058083891869, Entropy=0.6385494470596313, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02609768696129322, KL divergence=0.004713946022093296, Entropy=0.6603381037712097, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.008330625481903553, KL divergence=0.006082219537347555, Entropy=0.636555016040802, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.019807061180472374, KL divergence=0.004650793503969908, Entropy=0.680050790309906, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.027862409129738808, KL divergence=0.004231967497617006, Entropy=0.6579999923706055, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/65_Step-28854.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/65_Step-28854.ckpt']
Uploaded 3 files for checkpoint 65 in 0.56 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_65.pb
Best checkpoint number: 33, Last checkpoint number: 63
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'62'}
Training> Name=main_level/agent, Worker=0, Episode=1301, Total reward=51.02, Steps=28865, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1302, Total reward=158.03, Steps=28897, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1303, Total reward=101.03, Steps=28918, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1304, Total reward=56.02, Steps=28930, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1305, Total reward=118.03, Steps=28954, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1306, Total reward=38.01, Steps=28962, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1307, Total reward=142.03, Steps=28991, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1308, Total reward=220.04, Steps=29035, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1309, Total reward=170.03, Steps=29069, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1310, Total reward=95.02, Steps=29088, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1311, Total reward=645.14, Steps=29219, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1312, Total reward=436.1, Steps=29309, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1313, Total reward=512.11, Steps=29413, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1314, Total reward=208.04, Steps=29455, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1315, Total reward=133.03, Steps=29482, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1316, Total reward=438.1, Steps=29571, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1317, Total reward=184.04, Steps=29609, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1318, Total reward=165.03, Steps=29642, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1319, Total reward=413.09, Steps=29726, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=1320, Total reward=155.03, Steps=29757, Training iteration=65
Policy training> Surrogate loss=0.005452058278024197, KL divergence=0.0048582409508526325, Entropy=0.6488551497459412, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.008328529074788094, KL divergence=0.004525078926235437, Entropy=0.677862286567688, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.008282258175313473, KL divergence=0.002371961949393153, Entropy=0.6511545181274414, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.009622284211218357, KL divergence=0.006277349311858416, Entropy=0.6673908829689026, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.008955379016697407, KL divergence=0.003925083205103874, Entropy=0.668976366519928, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.015327614732086658, KL divergence=0.004677562974393368, Entropy=0.6712208390235901, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.01613454334437847, KL divergence=0.007814526557922363, Entropy=0.6818715929985046, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.012366478331387043, KL divergence=0.006242535077035427, Entropy=0.6806517839431763, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02217959240078926, KL divergence=0.004658492747694254, Entropy=0.6648224592208862, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.022961916401982307, KL divergence=0.004952289629727602, Entropy=0.6686221361160278, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/66_Step-29757.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/66_Step-29757.ckpt']
Uploaded 3 files for checkpoint 66 in 0.54 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_66.pb
Best checkpoint number: 33, Last checkpoint number: 64
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'63'}
Training> Name=main_level/agent, Worker=0, Episode=1321, Total reward=85.02, Steps=29774, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1322, Total reward=10.0, Steps=29776, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1323, Total reward=248.06, Steps=29827, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1324, Total reward=162.04, Steps=29860, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1325, Total reward=107.03, Steps=29882, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1326, Total reward=87.02, Steps=29900, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1327, Total reward=258.05, Steps=29952, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1328, Total reward=93.02, Steps=29971, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1329, Total reward=155.03, Steps=30002, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1330, Total reward=85.02, Steps=30019, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1331, Total reward=231.05, Steps=30066, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1332, Total reward=116.03, Steps=30090, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1333, Total reward=377.08, Steps=30166, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1334, Total reward=257.06, Steps=30219, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1335, Total reward=95.02, Steps=30239, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1336, Total reward=95.02, Steps=30259, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1337, Total reward=297.07, Steps=30320, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1338, Total reward=185.04, Steps=30357, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1339, Total reward=177.04, Steps=30393, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=1340, Total reward=30.01, Steps=30399, Training iteration=66
Policy training> Surrogate loss=-0.002013999270275235, KL divergence=0.0023753929417580366, Entropy=0.7732358574867249, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.003143705427646637, KL divergence=0.0059370542876422405, Entropy=0.7819297909736633, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.002234781626611948, KL divergence=0.0024175364524126053, Entropy=0.753498911857605, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.0011275552678853273, KL divergence=0.004567761905491352, Entropy=0.7600998878479004, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.012953514233231544, KL divergence=0.0035794400610029697, Entropy=0.7707071900367737, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.010504020377993584, KL divergence=0.003450243966653943, Entropy=0.7737406492233276, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.013605350628495216, KL divergence=0.0037159111816436052, Entropy=0.7658478617668152, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.013598324730992317, KL divergence=0.008397039957344532, Entropy=0.7292805910110474, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02040492370724678, KL divergence=0.0029438224155455828, Entropy=0.7661603093147278, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.021381039172410965, KL divergence=0.00446133129298687, Entropy=0.7537177801132202, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/67_Step-30399.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/67_Step-30399.ckpt']
Uploaded 3 files for checkpoint 67 in 0.53 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_67.pb
Best checkpoint number: 33, Last checkpoint number: 65
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'64'}
Training> Name=main_level/agent, Worker=0, Episode=1341, Total reward=120.02, Steps=30423, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1342, Total reward=57.02, Steps=30435, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1343, Total reward=122.03, Steps=30460, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1344, Total reward=123.03, Steps=30485, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1345, Total reward=106.03, Steps=30507, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1346, Total reward=48.01, Steps=30517, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1347, Total reward=300.06, Steps=30577, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1348, Total reward=186.04, Steps=30615, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1349, Total reward=147.03, Steps=30645, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1350, Total reward=470.1, Steps=30740, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1351, Total reward=184.05, Steps=30779, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1352, Total reward=568.12, Steps=30894, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1353, Total reward=612.13, Steps=31017, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1354, Total reward=183.04, Steps=31054, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1355, Total reward=397.09, Steps=31135, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1356, Total reward=427.09, Steps=31221, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1357, Total reward=101.02, Steps=31242, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1358, Total reward=155.03, Steps=31273, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1359, Total reward=215.04, Steps=31316, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=1360, Total reward=55.01, Steps=31327, Training iteration=67
Policy training> Surrogate loss=0.009477424435317516, KL divergence=0.004224914591759443, Entropy=0.7637471556663513, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.000705996819306165, KL divergence=0.006464212201535702, Entropy=0.7552614212036133, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.0012841315474361181, KL divergence=0.006109785288572311, Entropy=0.7194926142692566, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.0021587328519672155, KL divergence=0.007417204324156046, Entropy=0.769792377948761, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.017771122977137566, KL divergence=0.0032149755861610174, Entropy=0.7351706624031067, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.01818019151687622, KL divergence=0.003689265577122569, Entropy=0.7085895538330078, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.011555047705769539, KL divergence=0.006441914942115545, Entropy=0.7579956650733948, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.010681502521038055, KL divergence=0.003838054370135069, Entropy=0.7080853581428528, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.015681743621826172, KL divergence=0.007073839660733938, Entropy=0.7411636114120483, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.008027314208447933, KL divergence=0.007326985243707895, Entropy=0.7326191663742065, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/68_Step-31327.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/68_Step-31327.ckpt']
Uploaded 3 files for checkpoint 68 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_68.pb
Best checkpoint number: 33, Last checkpoint number: 66
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'65'}
Training> Name=main_level/agent, Worker=0, Episode=1361, Total reward=226.05, Steps=31373, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1362, Total reward=183.04, Steps=31410, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1363, Total reward=108.02, Steps=31432, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1364, Total reward=122.03, Steps=31457, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1365, Total reward=47.01, Steps=31467, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1366, Total reward=254.06, Steps=31519, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1367, Total reward=300.06, Steps=31579, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1368, Total reward=42.01, Steps=31588, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1369, Total reward=114.03, Steps=31612, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1370, Total reward=90.02, Steps=31630, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1371, Total reward=40.01, Steps=31638, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1372, Total reward=380.09, Steps=31716, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1373, Total reward=236.05, Steps=31764, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1374, Total reward=290.06, Steps=31823, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1375, Total reward=156.04, Steps=31855, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1376, Total reward=72.02, Steps=31870, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1377, Total reward=183.04, Steps=31907, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1378, Total reward=165.03, Steps=31940, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1379, Total reward=147.04, Steps=31971, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=1380, Total reward=138.03, Steps=31999, Training iteration=68
Policy training> Surrogate loss=-0.010473069734871387, KL divergence=0.0056962622329592705, Entropy=0.7366507649421692, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.0027379244565963745, KL divergence=0.015256841666996479, Entropy=0.7198411822319031, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.008534938097000122, KL divergence=0.0054149869829416275, Entropy=0.7665725946426392, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.013769343495368958, KL divergence=0.009985236451029778, Entropy=0.7430673837661743, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.01753498800098896, KL divergence=0.005357465706765652, Entropy=0.7664700746536255, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.006165993865579367, KL divergence=0.008392373099923134, Entropy=0.7445796132087708, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.004947400186210871, KL divergence=0.0018849184270948172, Entropy=0.7593628168106079, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.011501912027597427, KL divergence=0.011213360354304314, Entropy=0.7501388788223267, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.026287082582712173, KL divergence=0.0041170851327478886, Entropy=0.7524513006210327, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.013475269079208374, KL divergence=0.00711321085691452, Entropy=0.7456372976303101, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/69_Step-31999.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/69_Step-31999.ckpt']
Uploaded 3 files for checkpoint 69 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_69.pb
Best checkpoint number: 33, Last checkpoint number: 67
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'66'}
Training> Name=main_level/agent, Worker=0, Episode=1381, Total reward=378.08, Steps=32075, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1382, Total reward=177.04, Steps=32111, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1383, Total reward=92.02, Steps=32130, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1384, Total reward=62.02, Steps=32143, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1385, Total reward=42.01, Steps=32152, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1386, Total reward=52.01, Steps=32163, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1387, Total reward=289.06, Steps=32221, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1388, Total reward=230.05, Steps=32267, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1389, Total reward=165.03, Steps=32300, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1390, Total reward=249.06, Steps=32351, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1391, Total reward=495.1, Steps=32451, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1392, Total reward=291.07, Steps=32511, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1393, Total reward=302.06, Steps=32572, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1394, Total reward=241.05, Steps=32621, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1395, Total reward=143.04, Steps=32651, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1396, Total reward=459.09, Steps=32743, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1397, Total reward=52.01, Steps=32754, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1398, Total reward=88.02, Steps=32772, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1399, Total reward=206.05, Steps=32814, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=1400, Total reward=26.01, Steps=32820, Training iteration=69
Policy training> Surrogate loss=-0.012725849635899067, KL divergence=0.005469288676977158, Entropy=0.7208030223846436, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.007468800526112318, KL divergence=0.00683113606646657, Entropy=0.6919720768928528, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.023498347029089928, KL divergence=0.00354285747744143, Entropy=0.7278483510017395, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.018104394897818565, KL divergence=0.007021769881248474, Entropy=0.6883364319801331, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.020992150530219078, KL divergence=0.00715302862226963, Entropy=0.6919065117835999, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.020151318982243538, KL divergence=0.009009488858282566, Entropy=0.6995608806610107, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.022577904164791107, KL divergence=0.004034124780446291, Entropy=0.731743335723877, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.020315274596214294, KL divergence=0.007773240562528372, Entropy=0.6891835331916809, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.013285758905112743, KL divergence=0.004625540692359209, Entropy=0.7356739640235901, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.013613088987767696, KL divergence=0.009686780162155628, Entropy=0.707980215549469, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/70_Step-32820.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/70_Step-32820.ckpt']
Uploaded 3 files for checkpoint 70 in 0.50 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_70.pb
Best checkpoint number: 33, Last checkpoint number: 68
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'67'}
Training> Name=main_level/agent, Worker=0, Episode=1401, Total reward=80.02, Steps=32836, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1402, Total reward=152.03, Steps=32867, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1403, Total reward=137.03, Steps=32895, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1404, Total reward=61.02, Steps=32908, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1405, Total reward=113.03, Steps=32931, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1406, Total reward=37.01, Steps=32939, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1407, Total reward=416.09, Steps=33023, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1408, Total reward=159.04, Steps=33056, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1409, Total reward=155.03, Steps=33087, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1410, Total reward=80.02, Steps=33103, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1411, Total reward=386.08, Steps=33181, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1412, Total reward=156.04, Steps=33213, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1413, Total reward=307.06, Steps=33275, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1414, Total reward=217.05, Steps=33320, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1415, Total reward=367.08, Steps=33394, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1416, Total reward=67.02, Steps=33408, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1417, Total reward=290.06, Steps=33467, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1418, Total reward=47.01, Steps=33477, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1419, Total reward=135.03, Steps=33504, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=1420, Total reward=37.01, Steps=33512, Training iteration=70
Policy training> Surrogate loss=-0.0012131065595895052, KL divergence=0.004101501312106848, Entropy=0.7494009137153625, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.003039994742721319, KL divergence=0.005061905365437269, Entropy=0.7822517156600952, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.007057514972984791, KL divergence=0.004923949483782053, Entropy=0.7185260057449341, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.007103363983333111, KL divergence=0.002709459513425827, Entropy=0.7183326482772827, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0042684064246714115, KL divergence=0.004330629948526621, Entropy=0.7414101362228394, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=0.00479084113612771, KL divergence=0.003798300866037607, Entropy=0.7335221171379089, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.01782996766269207, KL divergence=0.0033412750344723463, Entropy=0.739272952079773, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.027837034314870834, KL divergence=0.0034985903184860945, Entropy=0.7440213561058044, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02204480767250061, KL divergence=0.0028851476963609457, Entropy=0.7158883810043335, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.01526590995490551, KL divergence=0.002766591729596257, Entropy=0.7385414242744446, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/71_Step-33512.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/71_Step-33512.ckpt']
Uploaded 3 files for checkpoint 71 in 0.50 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_71.pb
Best checkpoint number: 33, Last checkpoint number: 69
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'68'}
Training> Name=main_level/agent, Worker=0, Episode=1421, Total reward=155.04, Steps=33544, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1422, Total reward=197.04, Steps=33584, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1423, Total reward=178.04, Steps=33620, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1424, Total reward=97.02, Steps=33640, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1425, Total reward=94.03, Steps=33660, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1426, Total reward=52.01, Steps=33671, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1427, Total reward=288.07, Steps=33730, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1428, Total reward=233.06, Steps=33778, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1429, Total reward=158.03, Steps=33810, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1430, Total reward=212.05, Steps=33853, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1431, Total reward=181.04, Steps=33890, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1432, Total reward=551.11, Steps=34001, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1433, Total reward=343.07, Steps=34070, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1434, Total reward=450.09, Steps=34160, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1435, Total reward=247.05, Steps=34210, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1436, Total reward=325.06, Steps=34275, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1437, Total reward=82.02, Steps=34292, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1438, Total reward=149.03, Steps=34322, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1439, Total reward=500.11, Steps=34423, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=1440, Total reward=35.01, Steps=34430, Training iteration=71
Policy training> Surrogate loss=0.00464779045432806, KL divergence=0.0036103075835853815, Entropy=0.7310498952865601, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.006813011132180691, KL divergence=0.005459704902023077, Entropy=0.6457370519638062, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.0014280079631134868, KL divergence=0.0033801805693656206, Entropy=0.7161003947257996, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.0015672232257202268, KL divergence=0.0037464858032763004, Entropy=0.7003899216651917, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.011902001686394215, KL divergence=0.002852079691365361, Entropy=0.6852539777755737, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.010191093198955059, KL divergence=0.003321095835417509, Entropy=0.71941077709198, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.023780811578035355, KL divergence=0.0046104490756988525, Entropy=0.6953383088111877, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.013896816410124302, KL divergence=0.004746318329125643, Entropy=0.7028517723083496, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.016872992739081383, KL divergence=0.005178726278245449, Entropy=0.698042094707489, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.024533096700906754, KL divergence=0.00669779721647501, Entropy=0.7090178728103638, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/72_Step-34430.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/72_Step-34430.ckpt']
Uploaded 3 files for checkpoint 72 in 0.49 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_72.pb
Best checkpoint number: 33, Last checkpoint number: 70
Copying the frozen checkpoint from ./frozen_models/agent/model_33.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'69'}
Training> Name=main_level/agent, Worker=0, Episode=1441, Total reward=241.05, Steps=34479, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1442, Total reward=192.04, Steps=34518, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1443, Total reward=152.03, Steps=34549, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1444, Total reward=137.03, Steps=34577, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1445, Total reward=103.02, Steps=34598, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1446, Total reward=38.01, Steps=34606, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1447, Total reward=81.02, Steps=34623, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1448, Total reward=215.04, Steps=34666, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1449, Total reward=135.03, Steps=34693, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1450, Total reward=577.12, Steps=34809, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1451, Total reward=233.05, Steps=34856, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1452, Total reward=316.07, Steps=34920, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1453, Total reward=231.05, Steps=34967, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1454, Total reward=445.09, Steps=35056, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1455, Total reward=392.08, Steps=35135, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1456, Total reward=37.01, Steps=35143, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1457, Total reward=256.06, Steps=35195, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1458, Total reward=170.03, Steps=35229, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1459, Total reward=42.01, Steps=35238, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=1460, Total reward=30.01, Steps=35244, Training iteration=72
Policy training> Surrogate loss=0.0011561848223209381, KL divergence=0.006919473875313997, Entropy=0.617085874080658, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.0037065036594867706, KL divergence=0.002268928335979581, Entropy=0.6614975929260254, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.005551354493945837, KL divergence=0.003445723792538047, Entropy=0.6842101216316223, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.01734863966703415, KL divergence=0.0034953884314745665, Entropy=0.6386898159980774, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.0036921955179423094, KL divergence=0.002178773982450366, Entropy=0.6459292769432068, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.004094813019037247, KL divergence=0.0023727589286863804, Entropy=0.6482637524604797, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0026821743231266737, KL divergence=0.0025096863973885775, Entropy=0.6538258790969849, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.006805750075727701, KL divergence=0.0030073293019086123, Entropy=0.6512524485588074, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.024836784228682518, KL divergence=0.0028204545378684998, Entropy=0.6400112509727478, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.00963666383177042, KL divergence=0.0039320033974945545, Entropy=0.6599199771881104, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/73_Step-35244.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/73_Step-35244.ckpt']
Uploaded 3 files for checkpoint 73 in 0.57 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_73.pb
Best checkpoint number: 71, Last checkpoint number: 71
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'33'}
Training> Name=main_level/agent, Worker=0, Episode=1461, Total reward=80.02, Steps=35260, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1462, Total reward=368.08, Steps=35335, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1463, Total reward=118.03, Steps=35359, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1464, Total reward=133.04, Steps=35387, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1465, Total reward=124.03, Steps=35413, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1466, Total reward=96.02, Steps=35433, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1467, Total reward=119.03, Steps=35458, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1468, Total reward=655.15, Steps=35592, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1469, Total reward=494.11, Steps=35693, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1470, Total reward=90.02, Steps=35711, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1471, Total reward=22.01, Steps=35716, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1472, Total reward=604.14, Steps=35840, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1473, Total reward=354.08, Steps=35912, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1474, Total reward=293.06, Steps=35971, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1475, Total reward=364.08, Steps=36045, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1476, Total reward=142.03, Steps=36074, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1477, Total reward=254.05, Steps=36125, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1478, Total reward=137.03, Steps=36153, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1479, Total reward=204.04, Steps=36194, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=1480, Total reward=64.01, Steps=36207, Training iteration=73
Policy training> Surrogate loss=-0.001780248712748289, KL divergence=0.002020345302298665, Entropy=0.7694156765937805, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.004874981474131346, KL divergence=0.005762709770351648, Entropy=0.8063488602638245, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.000544959562830627, KL divergence=0.00413923617452383, Entropy=0.8044196367263794, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.006773035041987896, KL divergence=0.0058507854118943214, Entropy=0.8410547375679016, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.01412564143538475, KL divergence=0.003886134596541524, Entropy=0.7899766564369202, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.014111547730863094, KL divergence=0.005569829605519772, Entropy=0.8157936334609985, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.013193190097808838, KL divergence=0.00681713642552495, Entropy=0.79023677110672, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.01712621934711933, KL divergence=0.005345405079424381, Entropy=0.8047711253166199, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.01503781508654356, KL divergence=0.006345996167510748, Entropy=0.8352681398391724, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02148168534040451, KL divergence=0.006069136783480644, Entropy=0.793086588382721, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/74_Step-36207.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/74_Step-36207.ckpt']
Uploaded 3 files for checkpoint 74 in 0.55 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_74.pb
Best checkpoint number: 71, Last checkpoint number: 72
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'70'}
Training> Name=main_level/agent, Worker=0, Episode=1481, Total reward=60.01, Steps=36219, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1482, Total reward=192.04, Steps=36258, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1483, Total reward=230.06, Steps=36306, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1484, Total reward=98.02, Steps=36326, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1485, Total reward=47.01, Steps=36336, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1486, Total reward=43.01, Steps=36345, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1487, Total reward=283.07, Steps=36403, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1488, Total reward=238.05, Steps=36451, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1489, Total reward=46.01, Steps=36461, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1490, Total reward=90.02, Steps=36479, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1491, Total reward=133.03, Steps=36506, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1492, Total reward=337.07, Steps=36574, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1493, Total reward=276.07, Steps=36631, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1494, Total reward=203.04, Steps=36672, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1495, Total reward=182.04, Steps=36709, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1496, Total reward=293.06, Steps=36768, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1497, Total reward=97.02, Steps=36788, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1498, Total reward=325.08, Steps=36855, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1499, Total reward=200.04, Steps=36895, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=1500, Total reward=35.01, Steps=36902, Training iteration=74
Policy training> Surrogate loss=-0.0033167246729135513, KL divergence=0.004049879964441061, Entropy=0.8223254084587097, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=0.0005521699786186218, KL divergence=0.0063933394849300385, Entropy=0.7725900411605835, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.007502704858779907, KL divergence=0.0038993493653833866, Entropy=0.7480114698410034, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.0161677785217762, KL divergence=0.007242165505886078, Entropy=0.7792867422103882, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=0.001996457576751709, KL divergence=0.004065152257680893, Entropy=0.8044685125350952, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.01782999373972416, KL divergence=0.0036767288111150265, Entropy=0.7783416509628296, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0032444163225591183, KL divergence=0.003997051157057285, Entropy=0.7918182611465454, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0048217386938631535, KL divergence=0.004061835817992687, Entropy=0.8119403123855591, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.018925035372376442, KL divergence=0.004358990583568811, Entropy=0.7691552042961121, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.020253028720617294, KL divergence=0.002740363124758005, Entropy=0.808425784111023, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/75_Step-36902.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/75_Step-36902.ckpt']
Uploaded 3 files for checkpoint 75 in 0.59 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_75.pb
Best checkpoint number: 71, Last checkpoint number: 73
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'72'}
Training> Name=main_level/agent, Worker=0, Episode=1501, Total reward=208.07, Steps=36948, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1502, Total reward=48.01, Steps=36958, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1503, Total reward=130.03, Steps=36985, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1504, Total reward=127.03, Steps=37011, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1505, Total reward=107.03, Steps=37033, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1506, Total reward=87.02, Steps=37051, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1507, Total reward=280.06, Steps=37107, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1508, Total reward=233.05, Steps=37154, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1509, Total reward=140.03, Steps=37182, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1510, Total reward=90.02, Steps=37200, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1511, Total reward=231.05, Steps=37247, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1512, Total reward=327.07, Steps=37314, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1513, Total reward=240.05, Steps=37363, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1514, Total reward=400.09, Steps=37444, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1515, Total reward=268.06, Steps=37498, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1516, Total reward=317.07, Steps=37562, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1517, Total reward=365.07, Steps=37635, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1518, Total reward=186.04, Steps=37673, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1519, Total reward=120.02, Steps=37697, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=1520, Total reward=40.01, Steps=37705, Training iteration=75
Policy training> Surrogate loss=0.01022432092577219, KL divergence=0.002031090436503291, Entropy=0.7814938426017761, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.006758755538612604, KL divergence=0.004777505528181791, Entropy=0.732903003692627, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.007425311487168074, KL divergence=0.004903150722384453, Entropy=0.764944851398468, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.0023783084470778704, KL divergence=0.0045938328839838505, Entropy=0.7683737277984619, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026901086792349815, KL divergence=0.004141697194427252, Entropy=0.7501683235168457, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.017352363094687462, KL divergence=0.0050453501753509045, Entropy=0.7450523972511292, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0071983616799116135, KL divergence=0.005450593773275614, Entropy=0.7577540278434753, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.01642947643995285, KL divergence=0.004399061668664217, Entropy=0.7750909328460693, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02398749440908432, KL divergence=0.0038539606612175703, Entropy=0.7769232392311096, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02739100717008114, KL divergence=0.005848858971148729, Entropy=0.7591673731803894, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/76_Step-37705.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/76_Step-37705.ckpt']
Uploaded 3 files for checkpoint 76 in 0.55 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_76.pb
Best checkpoint number: 71, Last checkpoint number: 74
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'73'}
Training> Name=main_level/agent, Worker=0, Episode=1521, Total reward=38.01, Steps=37713, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1522, Total reward=174.04, Steps=37749, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1523, Total reward=108.02, Steps=37771, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1524, Total reward=48.01, Steps=37781, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1525, Total reward=47.01, Steps=37791, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1526, Total reward=206.05, Steps=37833, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1527, Total reward=522.11, Steps=37938, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1528, Total reward=146.04, Steps=37969, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1529, Total reward=160.03, Steps=38001, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1530, Total reward=547.11, Steps=38111, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1531, Total reward=158.03, Steps=38143, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1532, Total reward=132.03, Steps=38170, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1533, Total reward=629.13, Steps=38296, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1534, Total reward=391.08, Steps=38375, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1535, Total reward=197.04, Steps=38415, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1536, Total reward=93.02, Steps=38434, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1537, Total reward=245.05, Steps=38483, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1538, Total reward=137.03, Steps=38511, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1539, Total reward=105.02, Steps=38532, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=1540, Total reward=279.06, Steps=38589, Training iteration=76
Policy training> Surrogate loss=-0.006374177988618612, KL divergence=0.002655475866049528, Entropy=0.7000246047973633, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.0053995633497834206, KL divergence=0.003025156445801258, Entropy=0.7286603450775146, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.006366185378283262, KL divergence=0.006295034661889076, Entropy=0.7751342058181763, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.007194679696112871, KL divergence=0.004249427001923323, Entropy=0.7438892126083374, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0007541993400081992, KL divergence=0.0025908476673066616, Entropy=0.7317038178443909, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.009986602701246738, KL divergence=0.005345855839550495, Entropy=0.7701631188392639, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.01297555211931467, KL divergence=0.0039346227422356606, Entropy=0.711065411567688, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.005603369791060686, KL divergence=0.006260397378355265, Entropy=0.752872884273529, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.007441697176545858, KL divergence=0.0036991199012845755, Entropy=0.7231425642967224, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.022079288959503174, KL divergence=0.004154217429459095, Entropy=0.7535681128501892, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/77_Step-38589.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/77_Step-38589.ckpt']
Uploaded 3 files for checkpoint 77 in 0.53 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_77.pb
Best checkpoint number: 71, Last checkpoint number: 75
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'74'}
Training> Name=main_level/agent, Worker=0, Episode=1541, Total reward=55.01, Steps=38600, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1542, Total reward=32.01, Steps=38607, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1543, Total reward=223.05, Steps=38652, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1544, Total reward=37.01, Steps=38660, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1545, Total reward=68.02, Steps=38674, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1546, Total reward=58.01, Steps=38686, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1547, Total reward=866.18, Steps=38861, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1548, Total reward=78.02, Steps=38877, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1549, Total reward=149.04, Steps=38908, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1550, Total reward=90.02, Steps=38926, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1551, Total reward=152.04, Steps=38958, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1552, Total reward=353.07, Steps=39029, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1553, Total reward=88.02, Steps=39047, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1554, Total reward=257.06, Steps=39099, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1555, Total reward=91.02, Steps=39118, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1556, Total reward=86.02, Steps=39136, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1557, Total reward=225.05, Steps=39181, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1558, Total reward=47.01, Steps=39191, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1559, Total reward=111.03, Steps=39214, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=1560, Total reward=135.03, Steps=39241, Training iteration=77
Policy training> Surrogate loss=-0.004521640483289957, KL divergence=0.0019222941482439637, Entropy=0.7675734758377075, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.003102395683526993, KL divergence=0.012786991894245148, Entropy=0.7280518412590027, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.0016243442660197616, KL divergence=0.004065977409482002, Entropy=0.7290403842926025, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.010624292306602001, KL divergence=0.008954701013863087, Entropy=0.7675368189811707, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.018207931891083717, KL divergence=0.004982044920325279, Entropy=0.7461183667182922, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.011002054437994957, KL divergence=0.005896328948438168, Entropy=0.7411009073257446, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.017301995307207108, KL divergence=0.006221751682460308, Entropy=0.768420398235321, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.01676643267273903, KL divergence=0.00601864093914628, Entropy=0.7600386738777161, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02292705699801445, KL divergence=0.004832522477954626, Entropy=0.7581502795219421, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.033852219581604004, KL divergence=0.004066685680299997, Entropy=0.7545027732849121, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/78_Step-39241.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/78_Step-39241.ckpt']
Uploaded 3 files for checkpoint 78 in 0.56 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_78.pb
Best checkpoint number: 71, Last checkpoint number: 76
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'75'}
Training> Name=main_level/agent, Worker=0, Episode=1561, Total reward=85.02, Steps=39258, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1562, Total reward=157.04, Steps=39290, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1563, Total reward=87.02, Steps=39308, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1564, Total reward=168.04, Steps=39342, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1565, Total reward=105.03, Steps=39364, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1566, Total reward=38.01, Steps=39372, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1567, Total reward=309.06, Steps=39434, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1568, Total reward=157.04, Steps=39466, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1569, Total reward=291.08, Steps=39527, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1570, Total reward=88.03, Steps=39546, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1571, Total reward=768.17, Steps=39702, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1572, Total reward=292.07, Steps=39763, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1573, Total reward=142.03, Steps=39792, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1574, Total reward=425.08, Steps=39877, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1575, Total reward=428.9, Steps=39966, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1576, Total reward=337.07, Steps=40034, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1577, Total reward=463.1, Steps=40128, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1578, Total reward=261.06, Steps=40181, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1579, Total reward=125.03, Steps=40206, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=1580, Total reward=139.03, Steps=40234, Training iteration=78
Policy training> Surrogate loss=0.009996229782700539, KL divergence=0.007118467707186937, Entropy=0.7765399217605591, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.002201117342337966, KL divergence=0.00619677035138011, Entropy=0.770659327507019, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.00754994573071599, KL divergence=0.005923197139054537, Entropy=0.7965584397315979, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.00013810743985231966, KL divergence=0.004674532450735569, Entropy=0.7926807999610901, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.009130348451435566, KL divergence=0.003212671261280775, Entropy=0.8019376397132874, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.016576191410422325, KL divergence=0.003298263531178236, Entropy=0.7950423955917358, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.011573199182748795, KL divergence=0.004355784971266985, Entropy=0.7973096370697021, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.00729726767167449, KL divergence=0.0063412985764443874, Entropy=0.7672217488288879, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.017407095059752464, KL divergence=0.00872988160699606, Entropy=0.835558295249939, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.017782332375645638, KL divergence=0.00584745965898037, Entropy=0.7806081175804138, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/79_Step-40234.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/79_Step-40234.ckpt']
Uploaded 3 files for checkpoint 79 in 0.46 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_79.pb
Best checkpoint number: 71, Last checkpoint number: 77
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'76'}
Training> Name=main_level/agent, Worker=0, Episode=1581, Total reward=52.01, Steps=40245, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1582, Total reward=137.03, Steps=40273, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1583, Total reward=137.03, Steps=40301, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1584, Total reward=443.1, Steps=40391, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1585, Total reward=98.02, Steps=40411, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1586, Total reward=38.01, Steps=40419, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1587, Total reward=147.03, Steps=40449, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1588, Total reward=82.02, Steps=40466, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1589, Total reward=554.12, Steps=40579, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1590, Total reward=80.02, Steps=40595, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1591, Total reward=118.03, Steps=40619, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1592, Total reward=92.02, Steps=40638, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1593, Total reward=128.03, Steps=40664, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1594, Total reward=430.09, Steps=40750, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1595, Total reward=97.02, Steps=40770, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1596, Total reward=121.03, Steps=40795, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1597, Total reward=120.03, Steps=40820, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1598, Total reward=38.01, Steps=40828, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1599, Total reward=105.02, Steps=40849, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=1600, Total reward=40.01, Steps=40857, Training iteration=79
Policy training> Surrogate loss=-0.013853693380951881, KL divergence=0.0049858721904456615, Entropy=0.7853044867515564, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.03100821003317833, KL divergence=0.014012136496603489, Entropy=0.7198628187179565, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.005982061382383108, KL divergence=0.008598166517913342, Entropy=0.7605377435684204, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025184353813529015, KL divergence=0.008021810092031956, Entropy=0.7670908570289612, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029315313324332237, KL divergence=0.014130598865449429, Entropy=0.7417938709259033, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.04621097818017006, KL divergence=0.01026226207613945, Entropy=0.7548858523368835, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.022106721997261047, KL divergence=0.01624489203095436, Entropy=0.7379752397537231, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.027967385947704315, KL divergence=0.011840841732919216, Entropy=0.7536106109619141, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.039600685238838196, KL divergence=0.014861266128718853, Entropy=0.7382016777992249, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.05308737978339195, KL divergence=0.00829170923680067, Entropy=0.7614092230796814, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/80_Step-40857.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/80_Step-40857.ckpt']
Uploaded 3 files for checkpoint 80 in 0.52 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_80.pb
Best checkpoint number: 71, Last checkpoint number: 78
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'77'}
Training> Name=main_level/agent, Worker=0, Episode=1601, Total reward=70.01, Steps=40871, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1602, Total reward=263.06, Steps=40925, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1603, Total reward=117.03, Steps=40949, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1604, Total reward=163.04, Steps=40983, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1605, Total reward=57.02, Steps=40995, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1606, Total reward=82.02, Steps=41012, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1607, Total reward=269.06, Steps=41066, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1608, Total reward=38.01, Steps=41074, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1609, Total reward=150.03, Steps=41104, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1610, Total reward=83.02, Steps=41121, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1611, Total reward=106.03, Steps=41143, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1612, Total reward=87.02, Steps=41161, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1613, Total reward=298.07, Steps=41222, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1614, Total reward=470.09, Steps=41316, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1615, Total reward=347.07, Steps=41386, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1616, Total reward=32.01, Steps=41393, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1617, Total reward=225.05, Steps=41438, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1618, Total reward=259.06, Steps=41491, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1619, Total reward=41.01, Steps=41500, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=1620, Total reward=40.01, Steps=41508, Training iteration=80
Policy training> Surrogate loss=-0.0002539262059144676, KL divergence=0.00560952490195632, Entropy=0.7524540424346924, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.001401010900735855, KL divergence=0.004109281115233898, Entropy=0.8002265095710754, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.0037950589321553707, KL divergence=0.0027137589640915394, Entropy=0.7977678775787354, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=0.00291808252222836, KL divergence=0.009026525542140007, Entropy=0.7647978067398071, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.008336996659636497, KL divergence=0.003240905236452818, Entropy=0.818069577217102, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.010151821188628674, KL divergence=0.0033813905902206898, Entropy=0.785211980342865, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.01668042317032814, KL divergence=0.004263825248926878, Entropy=0.7876790761947632, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.007726275362074375, KL divergence=0.0026035229675471783, Entropy=0.7918568849563599, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.01810251548886299, KL divergence=0.003684879746288061, Entropy=0.7972773313522339, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.025618594139814377, KL divergence=0.002202246803790331, Entropy=0.7942841053009033, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/81_Step-41508.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/81_Step-41508.ckpt']
Uploaded 3 files for checkpoint 81 in 0.56 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_81.pb
Best checkpoint number: 71, Last checkpoint number: 79
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'78'}
Training> Name=main_level/agent, Worker=0, Episode=1621, Total reward=368.08, Steps=41582, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1622, Total reward=52.01, Steps=41593, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1623, Total reward=163.04, Steps=41626, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1624, Total reward=38.01, Steps=41634, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1625, Total reward=88.02, Steps=41652, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1626, Total reward=182.05, Steps=41690, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1627, Total reward=117.03, Steps=41714, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1628, Total reward=249.05, Steps=41764, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1629, Total reward=141.04, Steps=41794, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1630, Total reward=418.09, Steps=41878, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1631, Total reward=492.1, Steps=41977, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1632, Total reward=569.12, Steps=42092, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1633, Total reward=66.02, Steps=42106, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1634, Total reward=242.05, Steps=42155, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1635, Total reward=157.04, Steps=42187, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1636, Total reward=73.02, Steps=42202, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1637, Total reward=51.02, Steps=42213, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1638, Total reward=156.04, Steps=42245, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1639, Total reward=351.07, Steps=42316, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=1640, Total reward=140.03, Steps=42344, Training iteration=81
Policy training> Surrogate loss=0.007702951319515705, KL divergence=0.004654156044125557, Entropy=0.7135630249977112, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.008138447068631649, KL divergence=0.003814009018242359, Entropy=0.7025665044784546, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.0060223182663321495, KL divergence=0.0027628636453300714, Entropy=0.7254299521446228, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.00962819717824459, KL divergence=0.004027402028441429, Entropy=0.7141287922859192, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.016804412007331848, KL divergence=0.0035849192645400763, Entropy=0.7139618992805481, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.014695277437567711, KL divergence=0.003101784735918045, Entropy=0.7253910303115845, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.019210755825042725, KL divergence=0.005005623679608107, Entropy=0.7126710414886475, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.021336786448955536, KL divergence=0.0028187984135001898, Entropy=0.7418648600578308, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.022312315180897713, KL divergence=0.0047655608505010605, Entropy=0.7283627986907959, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.025117557495832443, KL divergence=0.004536744672805071, Entropy=0.7208756804466248, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/82_Step-42344.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/82_Step-42344.ckpt']
Uploaded 3 files for checkpoint 82 in 0.52 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_82.pb
Best checkpoint number: 71, Last checkpoint number: 80
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'79'}
Training> Name=main_level/agent, Worker=0, Episode=1641, Total reward=123.03, Steps=42369, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1642, Total reward=125.04, Steps=42396, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1643, Total reward=132.04, Steps=42424, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1644, Total reward=164.04, Steps=42458, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1645, Total reward=103.02, Steps=42479, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1646, Total reward=38.01, Steps=42487, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1647, Total reward=732.16, Steps=42635, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1648, Total reward=229.05, Steps=42682, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1649, Total reward=797.16, Steps=42842, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1650, Total reward=95.02, Steps=42861, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1651, Total reward=177.04, Steps=42897, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1652, Total reward=68.02, Steps=42911, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1653, Total reward=302.06, Steps=42972, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1654, Total reward=430.09, Steps=43059, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1655, Total reward=363.07, Steps=43132, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1656, Total reward=288.06, Steps=43190, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1657, Total reward=201.05, Steps=43231, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1658, Total reward=151.04, Steps=43262, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1659, Total reward=109.02, Steps=43284, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=1660, Total reward=150.03, Steps=43314, Training iteration=82
Policy training> Surrogate loss=0.006429871544241905, KL divergence=0.0033560560550540686, Entropy=0.6373186111450195, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.0007838202291168272, KL divergence=0.004440392833203077, Entropy=0.651049017906189, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.008653060533106327, KL divergence=0.0036710023414343596, Entropy=0.643510639667511, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.008710039779543877, KL divergence=0.004420336801558733, Entropy=0.6381807923316956, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.007463558577001095, KL divergence=0.0037649436853826046, Entropy=0.6306278705596924, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.017808642238378525, KL divergence=0.003287090454250574, Entropy=0.6302444338798523, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.01862809807062149, KL divergence=0.006565806455910206, Entropy=0.6463314890861511, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.014566048048436642, KL divergence=0.005261168349534273, Entropy=0.6213385462760925, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.01891165040433407, KL divergence=0.004195251036435366, Entropy=0.6343263983726501, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.021387213841080666, KL divergence=0.005625246092677116, Entropy=0.6679940819740295, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/83_Step-43314.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/83_Step-43314.ckpt']
Uploaded 3 files for checkpoint 83 in 0.54 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_83.pb
Best checkpoint number: 71, Last checkpoint number: 81
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'80'}
Training> Name=main_level/agent, Worker=0, Episode=1661, Total reward=90.02, Steps=43332, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1662, Total reward=303.06, Steps=43393, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1663, Total reward=128.03, Steps=43419, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1664, Total reward=102.02, Steps=43440, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1665, Total reward=118.03, Steps=43465, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1666, Total reward=48.01, Steps=43475, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1667, Total reward=307.06, Steps=43537, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1668, Total reward=209.05, Steps=43580, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1669, Total reward=147.03, Steps=43610, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1670, Total reward=100.02, Steps=43630, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1671, Total reward=46.01, Steps=43640, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1672, Total reward=422.09, Steps=43726, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1673, Total reward=226.05, Steps=43772, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1674, Total reward=429.09, Steps=43859, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1675, Total reward=405.09, Steps=43942, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1676, Total reward=38.01, Steps=43950, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1677, Total reward=184.04, Steps=43988, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1678, Total reward=166.04, Steps=44022, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1679, Total reward=205.05, Steps=44064, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=1680, Total reward=54.01, Steps=44075, Training iteration=83
Policy training> Surrogate loss=0.007609192747622728, KL divergence=0.005659834947437048, Entropy=0.6402541995048523, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014499249868094921, KL divergence=0.005007309373468161, Entropy=0.6398397088050842, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=0.00796885322779417, KL divergence=0.006140185985714197, Entropy=0.6463108658790588, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.017578303813934326, KL divergence=0.0023302570916712284, Entropy=0.6837246417999268, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.021368343383073807, KL divergence=0.007929157465696335, Entropy=0.6648402810096741, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.015447662211954594, KL divergence=0.0061353896744549274, Entropy=0.681954026222229, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.007574483752250671, KL divergence=0.007106070406734943, Entropy=0.6581534147262573, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.008636620827019215, KL divergence=0.006504213437438011, Entropy=0.6969888806343079, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.009288867004215717, KL divergence=0.007120628375560045, Entropy=0.6794143915176392, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.007449175696820021, KL divergence=0.007382812909781933, Entropy=0.6884888410568237, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/84_Step-44075.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/84_Step-44075.ckpt']
Uploaded 3 files for checkpoint 84 in 0.57 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_84.pb
Best checkpoint number: 71, Last checkpoint number: 82
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'81'}
Training> Name=main_level/agent, Worker=0, Episode=1681, Total reward=70.01, Steps=44089, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1682, Total reward=256.06, Steps=44141, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1683, Total reward=221.05, Steps=44186, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1684, Total reward=42.01, Steps=44195, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1685, Total reward=88.02, Steps=44213, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1686, Total reward=58.01, Steps=44225, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1687, Total reward=779.16, Steps=44382, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1688, Total reward=221.05, Steps=44427, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1689, Total reward=157.04, Steps=44459, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1690, Total reward=85.02, Steps=44476, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1691, Total reward=28.01, Steps=44482, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1692, Total reward=102.02, Steps=44503, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1693, Total reward=202.04, Steps=44544, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1694, Total reward=222.05, Steps=44589, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1695, Total reward=38.01, Steps=44597, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1696, Total reward=158.03, Steps=44629, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1697, Total reward=102.02, Steps=44650, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1698, Total reward=46.01, Steps=44660, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1699, Total reward=230.05, Steps=44706, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=1700, Total reward=45.01, Steps=44715, Training iteration=84
Policy training> Surrogate loss=0.0004464149533305317, KL divergence=0.0022179947700351477, Entropy=0.690403163433075, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.007478436920791864, KL divergence=0.005171839147806168, Entropy=0.6545525193214417, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.01219552755355835, KL divergence=0.004202795214951038, Entropy=0.6510683298110962, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.015097225084900856, KL divergence=0.004374492913484573, Entropy=0.6665874719619751, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.016767391934990883, KL divergence=0.005150961689651012, Entropy=0.6719642877578735, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02211105078458786, KL divergence=0.006419068668037653, Entropy=0.6576356291770935, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.023395637050271034, KL divergence=0.00691006937995553, Entropy=0.6463788747787476, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.023758063092827797, KL divergence=0.007875805720686913, Entropy=0.6731160879135132, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02448524534702301, KL divergence=0.005674135405570269, Entropy=0.6679062843322754, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02760922908782959, KL divergence=0.006842957343906164, Entropy=0.6610620617866516, training epoch=9, learning_rate=1e-05
INFO:tensorflow:./checkpoint/85_Step-44715.ckpt is not in all_model_checkpoint_paths. Manually adding it.
Checkpoint> Saving in path=['./checkpoint/85_Step-44715.ckpt']
Uploaded 3 files for checkpoint 85 in 0.51 seconds
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Converted 11 variables to const ops.
saved intermediate frozen graph: current/model/model_85.pb
Best checkpoint number: 71, Last checkpoint number: 83
Copying the frozen checkpoint from ./frozen_models/agent/model_71.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'82'}
Training> Name=main_level/agent, Worker=0, Episode=1701, Total reward=66.02, Steps=44729, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1702, Total reward=264.06, Steps=44783, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1703, Total reward=98.03, Steps=44804, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1704, Total reward=128.03, Steps=44830, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1705, Total reward=56.02, Steps=44842, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1706, Total reward=86.02, Steps=44860, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1707, Total reward=43.01, Steps=44869, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1708, Total reward=319.08, Steps=44935, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1709, Total reward=789.18, Steps=45096, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1710, Total reward=90.02, Steps=45114, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1711, Total reward=612.13, Steps=45238, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1712, Total reward=182.04, Steps=45275, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1713, Total reward=341.07, Steps=45344, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1714, Total reward=283.07, Steps=45402, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1715, Total reward=142.03, Steps=45431, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=1716, Total reward=169.04, Steps=45466, Training iteration=85
